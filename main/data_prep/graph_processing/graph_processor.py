import abc
import time

import numpy as np
import pandas as pd
from scipy.sparse import lil_matrix
from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit

from data_prep.graph_io import GraphIO
from utils.logging import calc_elapsed_time
from utils.rng import stochastic_method

EDGE_TYPE = {
    "self": 1,
    "doc-user": 2,
    "user-user": 3,
}


class GraphProcessor(GraphIO):
    def __init__(self, args, **super_kwargs):
        super().__init__(args, **super_kwargs)

    @abc.abstractmethod
    def get_user2docs(self):
        raise NotImplementedError

    @abc.abstractmethod
    def get_user2users(self):
        raise NotImplementedError

    def generate_node_id_mappings(self):
        start_time = time.time()

        self.print_step("Mapping docs and users to node id")

        flag = True
        if self._get_file_name("user2users").exists():
            self.log("Found existing `user2users`.")
        else:
            flag = False

        if self._get_file_name("doc2nodeid").exists():
            self.log("Found existing `doc2nodeid`.")
        else:
            flag = False

        if self._get_file_name("user2users").exists():
            self.log("Found existing `user2users`.")
        else:
            flag = False

        if self._get_file_name("user2nodeid").exists():
            self.log("Found existing `user2nodeid`.")
        else:
            flag = False

        if self._get_file_name("nodeid2type").exists():
            self.log("Found existing `nodeid2type`.")
        else:
            flag = False

        if flag and not self.overwrite:
            self.log(
                "All components generated by this function found and `overwrite` is False."
            )
            self.log("Skipping.")

        elif (not flag) or (flag and self.overwrite):
            if flag and self.overwrite:
                self.log(
                    "All components generated by this function found, but `overwrite` is True."
                )

            user2docs = self.get_user2docs()
            user2docs, user2users, sorted_users, degrees = self.get_user2users(
                user2docs
            )

            invalid_docs = self.load_file("invalid_docs")
            invalid_users = self.load_file("invalid_users")

            doc2users = self.load_file("doc2users")

            # Some filtering to make certain
            for doc_id, users in doc2users.items():
                doc2users[doc_id] = users - invalid_users
                if len(doc2users[doc_id]) == 0 and self.filter_out_isolated_docs:
                    invalid_docs.add(doc_id)

            isolated_docs = set.intersection(set(doc2users.keys()), invalid_docs)
            for doc_id in isolated_docs:
                del doc2users[doc_id]

            self.log(f"Num isolated docs post user truncation: {len(isolated_docs)}")

            for user_id in set.intersection(set(user2docs.keys()), invalid_users):
                del user2docs[user_id]

            for user_id in set.intersection(set(user2users.keys()), invalid_users):
                del user2users[user_id]

            # ======================================================================
            # Node id mappings
            # ======================================================================
            self.log("\nGenerating doc2id, user2id and nodeid2type mappings...")

            nodeid2type = list()

            # Process all the documents collected
            doc_dataset = self.load_file("doc_dataset")

            doc2nodeid = dict()
            node_id = 0
            for doc_id in doc_dataset["doc_id"]:
                if doc_id in invalid_docs:
                    continue

                doc2nodeid[doc_id] = node_id
                nodeid2type += [(node_id, "doc")]

                node_id += 1

            num_docs = len(doc2nodeid)

            user2nodeid = dict()
            num_incident_users = 0
            num_non_incident_users = 0
            for user_id in sorted_users:
                if user_id in invalid_users:
                    continue

                user2nodeid[user_id] = node_id

                if user_id in user2docs:
                    nodeid2type += [(node_id, "incident_user")]
                    num_incident_users += 1
                else:
                    nodeid2type += [(node_id, "non_incident_user")]
                    num_non_incident_users += 1

                node_id += 1

            # ======================================================================
            # Stats
            # ======================================================================
            self.log("\n+== Stats ==+")
            self.log("Node types:")
            self.log(f"Number of processed docs: {num_docs}")
            self.summary["Num incident users"] = num_incident_users
            self.log(f"Number of processed incident users: {num_incident_users}")
            self.summary["Num non-incident users"] = num_non_incident_users
            self.log(
                f"Number of processed non-incident users: {num_non_incident_users}"
            )

            self.log("\nUser Degrees:")
            user_degree_stats_summary = f"Mean: {np.mean(degrees):.2f}"
            user_degree_stats_summary += f" Std. Dev.: {np.std(degrees):.2f}"
            user_degree_stats_summary += (
                f" Quantiles: ["
                + ", ".join(
                    map(
                        lambda x: f"{int(x):d}",
                        np.quantile(degrees, [0, 0.25, 0.50, 0.75, 1]),
                    )
                )
                + "]"
            )
            user_degree_stats_summary += f" E[log(x)]={np.mean(np.log(degrees)):.2f}"
            user_degree_stats_summary += (
                f" exp(E[log(x)])={np.exp(np.mean(np.log(degrees))):.2f}"
            )
            self.summary["User degree stats summary"] = user_degree_stats_summary
            self.log(user_degree_stats_summary)

            self.save_file("user2docs", user2docs)
            self.save_file("user2users", user2users)

            self.save_file("nodeid2type", nodeid2type)
            self.save_file("doc2nodeid", doc2nodeid)
            self.save_file("user2nodeid", user2nodeid)

            self.save_file("invalid_users", invalid_users)
            self.save_file("invalid_docs", invalid_docs)

        self.save_file("summary")

        self.log("\nFinished mapping docs and users to node id.")
        end_time = time.time()
        hours, minutes, seconds = calc_elapsed_time(start_time, end_time)
        self.log(f"Time taken: {hours:02d}:{minutes:02d}:{seconds:02d}")

    def generate_adjacency_matrix(self):
        start_time = time.time()

        self.print_step("Generating edge list and adjacency matrix")

        doc2users = self.load_file("doc2users")
        user2docs = self.load_file("user2docs")
        user2users = self.load_file("user2users")
        doc2nodeid = self.load_file("doc2nodeid")
        user2nodeid = self.load_file("user2nodeid")
        invalid_users = self.load_file("invalid_users")

        num_docs = len(doc2nodeid)
        num_users = len(user2nodeid)
        num_nodes = num_docs + num_users

        adj_matrix = lil_matrix((num_nodes, num_nodes))
        edge_type = lil_matrix((num_nodes, num_nodes))
        edge_list = set()

        self.log("Writing self-self edges...")
        for i in range(num_nodes):
            adj_matrix[i, i] = 1
            edge_type[i, i] = EDGE_TYPE["self"]
            edge_list.add((i, i))

        failed_docs = set()

        self.log("Writing doc-user edges...")
        doc_user_edges = 0
        for doc_id, users in doc2users.items():
            try:
                doc_node_id = doc2nodeid[doc_id]
            except KeyError:
                failed_docs.add(doc_id)
                continue

            for user_id in users - invalid_users:
                user_node_id = user2nodeid[user_id]

                if (doc_node_id, user_node_id) in edge_list or (
                    user_node_id,
                    doc_node_id,
                ) in edge_list:
                    continue

                else:
                    adj_matrix[doc_node_id, user_node_id] = 1
                    adj_matrix[user_node_id, doc_node_id] = 1

                    edge_type[doc_node_id, user_node_id] = EDGE_TYPE["doc-user"]
                    edge_type[user_node_id, doc_node_id] = EDGE_TYPE["doc-user"]

                    edge_list.add((doc_node_id, user_node_id))
                    edge_list.add((user_node_id, doc_node_id))

                    doc_user_edges += 1

        self.log("Writing user-user edges...")
        user_user_edges = 0
        for user_a_id, users in user2users.items():
            if user_a_id in invalid_users:
                continue

            user_a_node_id = user2nodeid[user_a_id]

            for user_b_id in users - invalid_users:
                user_b_node_id = user2nodeid[user_b_id]

                if (user_a_node_id, user_b_node_id) in edge_list or (
                    user_b_node_id,
                    user_a_node_id,
                ) in edge_list:
                    continue

                else:
                    adj_matrix[user_a_node_id, user_b_node_id] = 1
                    adj_matrix[user_b_node_id, user_a_node_id] = 1

                    edge_type[user_a_node_id, user_b_node_id] = EDGE_TYPE["doc-user"]
                    edge_type[user_b_node_id, user_a_node_id] = EDGE_TYPE["doc-user"]

                    edge_list.add((user_a_node_id, user_b_node_id))
                    edge_list.add((user_b_node_id, user_a_node_id))

                    user_user_edges += 1

        self.log("\n+== Stats ==+")
        self.log(f"Recorded {num_nodes} unique self-self edges")
        self.summary["Num doc-user edges"] = doc_user_edges
        self.log(f"Recorded {doc_user_edges} unique doc-user edges")
        self.summary["Num user_use edges"] = user_user_edges
        self.log(f"Recorded {user_user_edges} unique user-user edges")
        self.summary["Num all edges"] = len(edge_list)
        self.log(f"Recorded {len(edge_list)} total edges")

        # To save ======================================================================
        self.save_file("adj_matrix", adj_matrix.tocsr())
        self.save_file("edge_type", edge_type.tocsr())
        self.save_file("edge_list", edge_list)

        self.save_file("summary")

        self.log("\nFinished building adjancency matrix.")
        end_time = time.time()
        hours, minutes, seconds = calc_elapsed_time(start_time, end_time)
        self.log(f"Time taken: {hours:02d}:{minutes:02d}:{seconds:02d}")

    @stochastic_method
    def split_documents(self):
        def class_count_table(arr):
            """
            Generates a little table with some stats for an array consisting of counts per class per split.
            Assumes array is of dimension [num_splits, num_classes].
            """

            arr = np.quantile(arr, [0, 0.5, 1.0], axis=0).T

            df = pd.DataFrame(arr)
            df.columns = ["Min", "Median", "Max"]
            df.index = [self.labels[l] for l in df.index]

            return df

        self.print_step("Creating Data Splits")
        start_time = time.time()

        invalid_docs = self.load_file("invalid_docs")
        doc_dataset = self.load_file("doc_dataset")
        doc_dataset_filtered = doc_dataset.filter(
            lambda row: row["doc_id"] not in invalid_docs
        )

        doc_id2row_id = {doc_id: i for i, doc_id in enumerate(doc_dataset["doc_id"])}

        split_indices = [
            {
                "train": [],
                "val": [],
                "test": [],
            }
            for _ in range(max(1, self.num_splits))
        ]

        if self.num_splits > 1:
            # print("Using nested cross-validation")
            # print(f"Will see a total of {self.num_splits}x{self.num_splits}={self.num_total_splits} experiments")

            self.log("Using cross-validation")
            self.log(
                f"Will see a total of {self.num_splits}x1={self.num_splits} experiments"
            )

            train_y = []
            val_y = []
            test_y = []

            strat_kfolds_outer = StratifiedKFold(
                n_splits=self.num_splits,
                random_state=self.seed,
                shuffle=True,
            )

            # Guarantees val set is same size as test set
            # But doesn't track across splits
            # In theory, validation sets could overlap except for samples in test set
            strat_kfolds_inner = StratifiedShuffleSplit(
                n_splits=1,
                test_size=(1 / (1 - 1 / self.num_splits)) * (1 / self.num_splits),
                random_state=self.seed,
            )

            split_id = 0
            for train_val_indices, test_indices in strat_kfolds_outer.split(
                np.zeros(len(doc_dataset_filtered["y"])),
                doc_dataset_filtered["y"],
            ):
                for train_indices, val_indices in strat_kfolds_inner.split(
                    np.zeros(len(train_val_indices)),
                    doc_dataset_filtered["y"][train_val_indices],
                ):
                    train_indices = train_val_indices[train_indices]
                    val_indices = train_val_indices[val_indices]

                    split_indices[split_id]["train"] = [
                        doc_id2row_id[doc_id]
                        for doc_id in np.array(doc_dataset_filtered["doc_id"])[
                            train_indices
                        ]
                    ]
                    split_indices[split_id]["val"] = [
                        doc_id2row_id[doc_id]
                        for doc_id in np.array(doc_dataset_filtered["doc_id"])[
                            val_indices
                        ]
                    ]
                    split_indices[split_id]["test"] = [
                        doc_id2row_id[doc_id]
                        for doc_id in np.array(doc_dataset_filtered["doc_id"])[
                            test_indices
                        ]
                    ]

                    split_id += 1

                    train_y += [doc_dataset_filtered["y"][train_indices]]
                    val_y += [doc_dataset_filtered["y"][val_indices]]
                    test_y += [doc_dataset_filtered["y"][test_indices]]

            self.log("\nTrain class counts:")
            class_counts_train = np.stack(list(map(np.bincount, train_y)), axis=0)
            df = class_count_table(class_counts_train)
            self.summary["train_class_counts"] = df.to_dict()
            self.log(df.to_string())

            self.log("\nValidation class counts:")
            class_counts_val = np.stack(list(map(np.bincount, val_y)), axis=0)
            df = class_count_table(class_counts_val)
            self.summary["val_class_counts"] = df.to_dict()
            self.log(df.to_string())

            self.log("\nTest class counts:")
            class_counts_test = np.stack(list(map(np.bincount, test_y)), axis=0)
            df = class_count_table(class_counts_test)
            self.summary["test_class_counts"] = df.to_dict()
            self.log(df.to_string())

        else:
            self.log("\n>>> WARNING: ONLY 1 SPLIT PROVIDED <<<\n")
            self.log("Assuming dataset is just used for test.")

            split_indices[0]["train"] = []
            split_indices[0]["val"] = []

            split_indices[0]["test"] = [
                doc_id2row_id[doc_id]
                for doc_id in np.array(doc_dataset_filtered["doc_id"])[
                    np.arange(stop=len(doc_dataset_filtered))
                ]
            ]

            self.log("\nTest class counts:")
            class_counts_test = np.bincount(doc_dataset["y"])
            df = class_count_table(class_counts_test[np.newaxis, :])
            self.summary["test_class_counts"] = df.to_dict()
            self.log(df.to_string())

        self.save_file(
            file_type="split_idx",
            obj=split_indices,
        )

        self.save_file(file_type="summary")

        self.log("\nFinished splitting documents.")
        end_time = time.time()
        hours, minutes, seconds = calc_elapsed_time(start_time, end_time)
        self.log(f"Time taken: {hours:02d}:{minutes:02d}:{seconds:02d}")
