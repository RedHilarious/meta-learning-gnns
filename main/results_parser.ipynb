{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {\"bbox_inches\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_destination = \"paper\"\n",
    "\n",
    "if figure_destination == \"paper\":\n",
    "    figsize = (3.03209, 0.22 * 9.72632)\n",
    "    fontsize_major = 9\n",
    "    fontsize_minor = 7\n",
    "    markersize_minor = 5\n",
    "    markersize_major = 6\n",
    "\n",
    "elif figure_destination == \"slide\":\n",
    "    figsize = (6.10, 4.87)\n",
    "    fontsize_major = 16\n",
    "    fontsize_minor = 11\n",
    "    markersize_minor = 4\n",
    "    markersize_major = 8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mono Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from utils.io import load_json_file\n",
    "from utils.logging import get_config_from_results_dir\n",
    "from utils.metrics import ci_multiplier\n",
    "\n",
    "\n",
    "def without(d, key):\n",
    "    new_d = d.copy()\n",
    "    new_d.pop(key)\n",
    "    return new_d\n",
    "\n",
    "\n",
    "def aggregate_dicts(results_dict, split: str, remove_nan: bool = True):\n",
    "    key_cut_off = len(split) + 1\n",
    "    \n",
    "    aggregated_dict = defaultdict(list)\n",
    "    for subdict in results_dict.values():\n",
    "        for k, v in subdict.items():\n",
    "            aggregated_dict[k[key_cut_off:]] += [v]\n",
    "\n",
    "    aggregated_dict = dict(aggregated_dict)\n",
    "\n",
    "    drop_keys = set()\n",
    "    for k, v in aggregated_dict.items():\n",
    "        if np.isnan(v[0]) and remove_nan:\n",
    "            drop_keys.add(k)\n",
    "            continue\n",
    "\n",
    "        N = len(v)\n",
    "\n",
    "        mean = np.mean(v)\n",
    "        var = np.var(v)\n",
    "        se = math.sqrt(var / N)\n",
    "        ub = mean + ci_multiplier(N, alpha=0.10) * se\n",
    "        lb = mean - ci_multiplier(N, alpha=0.10) * se\n",
    "\n",
    "        agg_results = (mean, var, se, lb, ub, N)\n",
    "\n",
    "        aggregated_dict[k] = agg_results\n",
    "\n",
    "    return {k: v for k, v in aggregated_dict.items() if k not in drop_keys}\n",
    "\n",
    "\n",
    "def aggregate_weighted_dicts(results_dict, split: str, remove_nan: bool = True):\n",
    "    key_cut_off = len(split) + 1\n",
    "    \n",
    "    aggregated_dict = defaultdict(list)\n",
    "    for subdict in results_dict.values():\n",
    "        for k, v in subdict.items():\n",
    "            aggregated_dict[k[key_cut_off:]] += [v]\n",
    "\n",
    "    aggregated_dict = dict(aggregated_dict)\n",
    "\n",
    "    drop_keys = set()\n",
    "    for k, v in aggregated_dict.items():\n",
    "        if np.isnan(v[0][0]) and remove_nan:\n",
    "            drop_keys.add(k)\n",
    "            continue\n",
    "\n",
    "\n",
    "        v_arr = np.stack(v)\n",
    "        weights = 1/v_arr[:, 1]\n",
    "        weights_sum = np.sum(weights)\n",
    "        N = np.sum(v_arr[:, 2])\n",
    "\n",
    "        mean = np.sum(weights * v_arr[:, 0]) / weights_sum\n",
    "        var = 1 / weights_sum\n",
    "        se = math.sqrt(var)\n",
    "        ub = mean + ci_multiplier(N, alpha=0.10) * se\n",
    "        lb = mean - ci_multiplier(N, alpha=0.10) * se\n",
    "\n",
    "        agg_results = (mean, var, se, lb, ub, N)\n",
    "\n",
    "        aggregated_dict[k] = agg_results\n",
    "\n",
    "    return {k: v for k, v in aggregated_dict.items() if k not in drop_keys}\n",
    "\n",
    "\n",
    "split = \"test\"\n",
    "filters = {\"dataset\": \"gossipcop\", \"top_users_excluded\": 1, \"checkpoint\": \"230426_roberta_proto\", \"version\": None}\n",
    "metrics = [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\", \"macro_aupr\"]\n",
    "\n",
    "checkpoint_results_weighted = defaultdict(dict)\n",
    "checkpoint_results = defaultdict(dict)\n",
    "checkpoint_hparams = dict()\n",
    "for i, (top_dir, sub_dirs, sub_files) in enumerate(os.walk(\"../results\")):\n",
    "    top_dir_path = Path(top_dir)\n",
    "\n",
    "    if len(sub_files) > 0 and top_dir_path.parts[-1] != \"summary\":\n",
    "        base_config = get_config_from_results_dir(top_dir)\n",
    "\n",
    "        continue_flag = False\n",
    "        for fk, fv in filters.items():\n",
    "            if base_config[fk] != fv:\n",
    "                continue_flag = True\n",
    "\n",
    "        if \"sweep\" in base_config[\"checkpoint\"]:\n",
    "            continue_flag = True\n",
    "\n",
    "        if continue_flag:\n",
    "            continue\n",
    "\n",
    "        for sub_file in sub_files:\n",
    "            if \"hparams\" in sub_file:\n",
    "                with open(top_dir_path / sub_file, \"rb\") as f:\n",
    "                    hparams = pickle.load(f)\n",
    "\n",
    "            if f\"{split}_\" in sub_file:\n",
    "                results = load_json_file(top_dir_path / sub_file)\n",
    "\n",
    "                print(\"1\")\n",
    "\n",
    "        base_config_tuple = tuple(\n",
    "            [\n",
    "                *sorted(without(base_config, \"fold\").items(), key=lambda x: x[0]),\n",
    "                (\"meta learner\", hparams['learning_hparams']['meta_learner']),\n",
    "                (\"reset head\", hparams['learning_hparams']['reset_classifier']),\n",
    "                (\"inner updates\", hparams['learning_hparams']['n_inner_updates']),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        checkpoint_results[base_config_tuple][base_config[\"fold\"]] = results\n",
    "        checkpoint_hparams[base_config_tuple] = hparams\n",
    "\n",
    "        if \"episodic_khop\" in base_config[\"structure\"]:\n",
    "            results_with_weights = {}\n",
    "            for k, v in results.items():\n",
    "                if \"std\" not in k and \"se\" not in k and k + \"_se\" in results:\n",
    "                    mean_effect = v\n",
    "                    # In fixed-effects meta-analysis, the standard error\n",
    "                    # is assumed to be an estimator for the variance\n",
    "                    var = math.pow(results[k + \"_se\"], 2)\n",
    "                    N = results[f\"{split}/eval_iterations\"]\n",
    "\n",
    "                    results_with_weights[k] = (mean_effect, var, N)\n",
    "\n",
    "            checkpoint_results_weighted[base_config_tuple][\n",
    "                base_config[\"fold\"]\n",
    "            ] = results_with_weights\n",
    "\n",
    "checkpoint_results = dict(checkpoint_results)\n",
    "checkpoint_results_weighted = dict(checkpoint_results_weighted)\n",
    "\n",
    "for k, v in checkpoint_results.items():\n",
    "    aggregate_stats = aggregate_dicts(v, split=split, remove_nan=True)\n",
    "    checkpoint_results[k] = [\n",
    "        (k, aggregate_stats[k]) for k in metrics if k in aggregate_stats\n",
    "    ]\n",
    "\n",
    "    if k in checkpoint_results_weighted:\n",
    "        aggregate_weighted_stats = aggregate_weighted_dicts(checkpoint_results_weighted[k], split=split, remove_nan=True)\n",
    "        checkpoint_results_weighted[k] = [\n",
    "            (k, aggregate_weighted_stats[k])\n",
    "            for k in metrics\n",
    "            if k in aggregate_stats\n",
    "        ]\n",
    "\n",
    "#checkpoint_results = [(k, v) for k, v in checkpoint_results.items()]\n",
    "#checkpoint_results_weighted = [(k, v) for k, v in checkpoint_results_weighted.items()]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "records = []\n",
    "for entry_record, entries in checkpoint_results.items():\n",
    "    \n",
    "    entry_record_ = dict(entry_record)\n",
    "    \n",
    "    if \"episodic\" in entry_record_[\"structure\"]:\n",
    "        entries = checkpoint_results_weighted[entry_record]\n",
    "    \n",
    "    for metric, (mean_val, _, _, lb, ub, _) in entries:\n",
    "        entry_record_.update({\n",
    "            metric: mean_val,\n",
    "            metric + \"_lb\": lb,\n",
    "            metric + \"_ub\": ub,\n",
    "        })\n",
    "\n",
    "    records.append(entry_record_)\n",
    "\n",
    "checkpoint_results = pd.DataFrame.from_records(\n",
    "    records\n",
    ")\n",
    "\n",
    "#checkpoint_results = checkpoint_results.loc[:, (checkpoint_results != checkpoint_results.iloc[0]).any()] \n",
    "\n",
    "checkpoint_results.to_clipboard(excel=True,)\n",
    "\n",
    "checkpoint_results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.io import load_json_file\n",
    "from utils.logging import get_config_from_results_dir\n",
    "from utils.metrics import ci_multiplier\n",
    "\n",
    "def without(d, key):\n",
    "    new_d = d.copy()\n",
    "    new_d.pop(key)\n",
    "    return new_d\n",
    "\n",
    "def aggregate_dicts(results_dict, alpha, split, remove_nan: bool = True):\n",
    "\n",
    "    aggregated_dict = defaultdict(list)\n",
    "    for subdict in results_dict.values():\n",
    "        for k, v in subdict.items():\n",
    "            aggregated_dict[k[len(split) + 1:]] += [v]\n",
    "\n",
    "    aggregated_dict = dict(aggregated_dict)\n",
    "\n",
    "    drop_keys = set()\n",
    "    for k, v in aggregated_dict.items():\n",
    "        if np.isnan(v[0]) and remove_nan:\n",
    "            drop_keys.add(k)\n",
    "            continue\n",
    "\n",
    "        N = len(v)\n",
    "\n",
    "        mean = np.mean(v)\n",
    "        var = np.var(v)\n",
    "        se = math.sqrt(var / N)\n",
    "        ub = mean + ci_multiplier(alpha=alpha, N=N) * se\n",
    "        lb = mean - ci_multiplier(alpha=alpha, N=N) * se\n",
    "\n",
    "        agg_results = (mean, var, se, lb, ub, N)\n",
    "\n",
    "        aggregated_dict[k] = agg_results\n",
    "\n",
    "    return {k: v for k, v in aggregated_dict.items() if k not in drop_keys}\n",
    "\n",
    "def aggregate_weighted_dicts(results_dict, alpha, split, remove_nan: bool = True):\n",
    "    aggregated_dict = defaultdict(list)\n",
    "    for subdict in results_dict.values():\n",
    "        for k, v in subdict.items():\n",
    "            aggregated_dict[k[len(split) + 1:]] += [v]\n",
    "\n",
    "    aggregated_dict = dict(aggregated_dict)\n",
    "\n",
    "    drop_keys = set()\n",
    "    for k, v in aggregated_dict.items():\n",
    "        if np.isnan(v[0][0]) and remove_nan:\n",
    "            drop_keys.add(k)\n",
    "            continue\n",
    "\n",
    "        v_arr = np.stack(v)\n",
    "        weights = 1 / v_arr[:, 1]\n",
    "\n",
    "        weights[np.isinf(weights)] = 0\n",
    "        weights[np.isneginf(weights)] = 0\n",
    "        weights[np.isnan(weights)] = 0\n",
    "\n",
    "        weights_sum = np.sum(weights)\n",
    "\n",
    "        if weights_sum == 0:\n",
    "            #agg_results = (np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 0)\n",
    "\n",
    "            mean = np.mean(v_arr[:, 0])\n",
    "            var = np.array(0.0)\n",
    "            se = np.array(0.0)\n",
    "\n",
    "            N = np.sum(v_arr[:, 2])\n",
    "\n",
    "            ub = mean + ci_multiplier(N=N, alpha=alpha) * se\n",
    "            lb = mean - ci_multiplier(N=N, alpha=alpha) * se\n",
    "\n",
    "            agg_results = (mean, var, se, lb, ub, N)\n",
    "\n",
    "        else:\n",
    "            mean = np.sum(weights * v_arr[:, 0]) / weights_sum\n",
    "            var = 1 / weights_sum\n",
    "            se = math.sqrt(var)\n",
    "\n",
    "            N = np.sum(v_arr[:, 2])\n",
    "\n",
    "            ub = mean + ci_multiplier(N=N, alpha=alpha) * se\n",
    "            lb = mean - ci_multiplier(N=N, alpha=alpha) * se\n",
    "\n",
    "            agg_results = (mean, var, se, lb, ub, N)\n",
    "\n",
    "        aggregated_dict[k] = agg_results\n",
    "\n",
    "    return {k: v for k, v in aggregated_dict.items() if k not in drop_keys}\n",
    "\n",
    "def get_weighted_results_table(filters: dict, metrics: list, split: str, alpha: float = 0.10):\n",
    "    raw_values = []\n",
    "    checkpoint_results_weighted = defaultdict(dict)\n",
    "    checkpoint_results = defaultdict(dict)\n",
    "    for i, (top_dir, sub_dirs, sub_files) in enumerate(os.walk(\"../results\")):\n",
    "        top_dir_path = Path(top_dir)\n",
    "\n",
    "        if len(sub_files) > 0 and top_dir_path.parts[-1] != \"summary\":\n",
    "            base_config = get_config_from_results_dir(top_dir)\n",
    "\n",
    "            continue_flag = False\n",
    "            for fk, fv in filters.items():\n",
    "                if base_config[fk] != fv:\n",
    "                    continue_flag = True\n",
    "\n",
    "            if \"sweep\" in base_config[\"checkpoint\"]:\n",
    "                continue_flag = True\n",
    "\n",
    "            if continue_flag:\n",
    "                continue\n",
    "\n",
    "            for sub_file in sub_files:\n",
    "\n",
    "                if f\"{split}_eval\" in sub_file:\n",
    "                    results = load_json_file(top_dir_path / sub_file)\n",
    "\n",
    "                    bracktets_pattern = r\"\\[(.*?)\\]\"\n",
    "\n",
    "                    eval_str_parts = re.split(r\"\\_(?![^[]*])\", sub_file)\n",
    "\n",
    "                    k = int(re.search(bracktets_pattern, eval_str_parts[2]).group(1))\n",
    "                    n_updates = int(\n",
    "                        re.search(bracktets_pattern, eval_str_parts[3]).group(1)\n",
    "                    )\n",
    "                    inner_lr = float(\n",
    "                        re.search(bracktets_pattern, eval_str_parts[4]).group(1)\n",
    "                    )\n",
    "                    inner_head_lr = float(\n",
    "                        re.search(bracktets_pattern, eval_str_parts[5]).group(1)\n",
    "                    )\n",
    "\n",
    "                    class_weights = tuple(\n",
    "                        map(\n",
    "                            float,\n",
    "                            re.search(bracktets_pattern, eval_str_parts[6])\n",
    "                            .group(1)[8:]\n",
    "                            .split(\", \"),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    if len(eval_str_parts) == 8:\n",
    "                        budget = int(\n",
    "                            re.search(bracktets_pattern, eval_str_parts[7]).group(1)\n",
    "                        )\n",
    "                    else:\n",
    "                        budget = 2048     \n",
    "\n",
    "                    eval_tuple = (\n",
    "                        (\"k\", k),\n",
    "                        (\"n_updates\", n_updates),\n",
    "                        (\"inner_lr\", inner_lr),\n",
    "                        (\"inner_head_lr\", inner_head_lr),\n",
    "                        (\"class_weights\", class_weights),\n",
    "                        (\"budget\", budget),\n",
    "                        (\"reset\", \"reset\" in base_config[\"version\"]),\n",
    "                        (\"user_init\", \"user_init\" in base_config[\"version\"]),\n",
    "                    )\n",
    "\n",
    "                base_config_tuple = tuple(eval_tuple)\n",
    "\n",
    "                checkpoint_results[base_config_tuple][base_config[\"fold\"]] = results\n",
    "\n",
    "                raw_values.append(\n",
    "                    base_config | dict(eval_tuple) | results\n",
    "                )\n",
    "\n",
    "                if \"episodic_khop\" in base_config[\"structure\"]:\n",
    "                    results_with_weights = {}\n",
    "\n",
    "                    for k, v in results.items():\n",
    "                        if \"iterations\" in k:\n",
    "                            N = v\n",
    "                            break\n",
    "\n",
    "                    for k, v in results.items():\n",
    "                        if \"std\" not in k and \"se\" not in k and k + \"_se\" in results:\n",
    "                            mean_effect = v\n",
    "                            # In fixed-effects meta-analysis, the standard error\n",
    "                            # is assumed to be an estimator for the variance\n",
    "                            var = math.pow(results[k + \"_se\"], 2)\n",
    "\n",
    "                            results_with_weights[k] = (mean_effect, var, N)\n",
    "\n",
    "                    checkpoint_results_weighted[base_config_tuple][\n",
    "                        base_config[\"fold\"]\n",
    "                    ] = results_with_weights\n",
    "\n",
    "    checkpoint_results = dict(checkpoint_results)\n",
    "    checkpoint_results_weighted = dict(checkpoint_results_weighted)\n",
    "    \n",
    "    for k, v in checkpoint_results.items():\n",
    "        aggregate_stats = aggregate_dicts(v, alpha, split=split, remove_nan=False)\n",
    "        checkpoint_results[k] = [\n",
    "            (kk, aggregate_stats[kk]) for kk in metrics if kk in aggregate_stats\n",
    "        ]\n",
    "\n",
    "        if k in checkpoint_results_weighted:\n",
    "            aggregate_weighted_stats = aggregate_weighted_dicts(\n",
    "                checkpoint_results_weighted[k], alpha, split=split, remove_nan=False\n",
    "            )\n",
    "            checkpoint_results_weighted[k] = [\n",
    "                (kk, aggregate_weighted_stats[kk]) for kk in metrics if kk in aggregate_weighted_stats\n",
    "            ]\n",
    "\n",
    "    checkpoint_results = [(k, v) for k, v in checkpoint_results.items()]\n",
    "\n",
    "    records = []\n",
    "    for i in range(len(checkpoint_results)):\n",
    "        entry_record = dict(checkpoint_results[i][0])\n",
    "        for entry in checkpoint_results[i][1]:\n",
    "            entry_record.update({\n",
    "                entry[0]: entry[1][0],\n",
    "                entry[0] + \"_var\": entry[1][1],\n",
    "                entry[0] + \"_se\":  entry[1][2],\n",
    "                entry[0] + \"_lb\":  entry[1][3],\n",
    "                entry[0] + \"_ub\":  entry[1][4],\n",
    "                entry[0] + \"_N\":   entry[1][5],\n",
    "            })\n",
    "\n",
    "        records.append(entry_record)\n",
    "\n",
    "\n",
    "    checkpoint_results = pd.DataFrame.from_records(\n",
    "        checkpoint_results\n",
    "    )\n",
    "\n",
    "    checkpoint_results_weighted = [(k, v) for k, v in checkpoint_results_weighted.items()]\n",
    "\n",
    "    records = []\n",
    "    for i in range(len(checkpoint_results_weighted)):\n",
    "        entry_record = dict(checkpoint_results_weighted[i][0])\n",
    "        for entry in checkpoint_results_weighted[i][1]:\n",
    "            entry_record.update({\n",
    "                entry[0]: entry[1][0],\n",
    "                entry[0] + \"_var\": entry[1][1],\n",
    "                entry[0] + \"_se\":  entry[1][2],\n",
    "                entry[0] + \"_lb\":  entry[1][3],\n",
    "                entry[0] + \"_ub\":  entry[1][4],\n",
    "                entry[0] + \"_N\":   entry[1][5],\n",
    "            })\n",
    "\n",
    "        records.append(entry_record)\n",
    "\n",
    "    checkpoint_results_weighted = pd.DataFrame.from_records(\n",
    "        records\n",
    "    )\n",
    "    #).sort_values(\n",
    "    #    by=[\"k\", \"n_updates\", \"inner_head_lr\", \"inner_lr\", \"class_weights\"]\n",
    "    #)\n",
    "\n",
    "    return raw_values, checkpoint_results, checkpoint_results_weighted\n",
    "\n",
    "def normalize_by_group(df, by, metric, remove: set = set()):\n",
    "    \n",
    "    sub_df = df[[*by, f\"{metric}\", f\"{metric}_se\", f\"{metric}_N\"]].copy(deep=True)\n",
    "    sub_df[f\"{metric}_w\"] = np.power(sub_df[f\"{metric}_se\"], -2)\n",
    "    \n",
    "    grouped_sub_df = sub_df.groupby(by=by, as_index=True, group_keys=False)\n",
    "    \n",
    "    agg_variance = (grouped_sub_df        \n",
    "        .apply(lambda x: 1 / x[f\"{metric}_w\"].sum())\n",
    "        .reset_index(level=[], name=f\"{metric}_var\")\n",
    "    )\n",
    "\n",
    "    agg_mean = (grouped_sub_df        \n",
    "        .apply(lambda x: (x[f\"{metric}\"] * x[f\"{metric}_w\"]).sum())\n",
    "        .reset_index(level=[], name=f\"{metric}\")\n",
    "    )\n",
    "\n",
    "    agg_N = (grouped_sub_df        \n",
    "        .apply(lambda x: x[f\"{metric}_N\"].sum())\n",
    "        .reset_index(level=[], name=f\"{metric}_N\")\n",
    "    )\n",
    "\n",
    "    agg_values = agg_mean.join(\n",
    "        other=[agg_variance, agg_N]\n",
    "    )\n",
    "\n",
    "    agg_values[f\"{metric}\"] = agg_values[f\"{metric}\"] * agg_values[f\"{metric}_var\"]\n",
    "\n",
    "    agg_values[f\"{metric}_se\"] = np.sqrt(agg_values[f\"{metric}_var\"])\n",
    "\n",
    "    agg_values = agg_values.join(\n",
    "        (agg_values\n",
    "            .apply(\n",
    "                lambda x: [\n",
    "                    x[f\"{metric}\"] - ci_multiplier(N=x[f\"{metric}_N\"], alpha=0.1) * x[f\"{metric}_se\"],\n",
    "                    x[f\"{metric}\"] + ci_multiplier(N=x[f\"{metric}_N\"], alpha=0.1) * x[f\"{metric}_se\"]\n",
    "                    ],\n",
    "                axis=1,\n",
    "                result_type='expand',\n",
    "                )\n",
    "            .rename(\n",
    "                columns={0: f\"{metric}_lb\", 1: f\"{metric}_ub\"}\n",
    "                )\n",
    "        ),\n",
    "        \n",
    "    )\n",
    "\n",
    "    agg_values = agg_values[\n",
    "        agg_values.columns\n",
    "        .drop(\n",
    "            [\n",
    "                col_name\n",
    "                for condition in remove\n",
    "                for col_name in agg_values.filter(regex=condition).columns.tolist()\n",
    "                ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return agg_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "checkpoint_cmap = {\n",
    "    \"zero_shot_transfer\": cmap(0),\n",
    "    \"subgraphs\": cmap(0),\n",
    "    \"maml_lh\": cmap(1),\n",
    "    \"maml_rh\": cmap(2),\n",
    "    \"prototypical\": cmap(3),\n",
    "    \"protomaml\": cmap(4),\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"subgraphs\": [\"vb49pmtr\", \"8kixi0s8\", \"vzswebea\", \"a4xe91oq\", \"6syunm30\"],\n",
    "    \"maml_lh\": [\"zqhx6x3b\", \"11pt2nis\", \"ruy4hp9o\", \"nlfyh80j\", \"06pfaw4f\"],\n",
    "    \"maml_rh\": [\"rpu3r1rm\", \"35neqj40\", \"xycl2xbl\", \"9hdrq4an\", \"905yf717\"],\n",
    "    \"prototypical\": [\"yjnx3e9w\", \"5e0vvr04\", \"br7qcerq\", \"6c8tvtwn\", \"ahp19u65\"],\n",
    "    \"protomaml\": [\"9o4wp36l\", \"ouxd7twt\", \"aimkj4sa\", \"euh2mnqo\", \"p33ybhsn\"],\n",
    "}\n",
    "\n",
    "metrics = [\n",
    "    \"supp_improvement\",\n",
    "    \"f1_0\",\n",
    "    \"aupr_0\",\n",
    "    \"f1_1\",\n",
    "    \"aupr_1\",\n",
    "    \"f1_2\",\n",
    "    \"aupr_2\",\n",
    "    \"mcc\",\n",
    "    \"macro_aupr\",\n",
    "]\n",
    "\n",
    "all_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"twitterHateSpeech\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        all_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#all_checkpoint_results = all_checkpoint_results[0]\n",
    "all_checkpoint_results = pd.concat(all_checkpoint_results).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out some early attempts with too high learning rate\n",
    "# These are noisy data points\n",
    "all_checkpoint_results[(all_checkpoint_results[\"n_updates\"] == 25)] = (\n",
    "    all_checkpoint_results[(all_checkpoint_results[\"n_updates\"] == 25)]\n",
    "    .sort_values(by=[\"learning_algorithm\", \"checkpoint\", \"k\", \"inner_lr\", \"inner_head_lr\"])\n",
    "    .drop_duplicates(subset=[\"learning_algorithm\", \"checkpoint\", \"k\"])\n",
    ")\n",
    "\n",
    "all_checkpoint_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=all_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_values = defaultdict(list)\n",
    "custom_lines = []\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "for k in all_k_shots:\n",
    "    matching_k = all_checkpoint_results[\"k\"] == k\n",
    "\n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (\n",
    "            all_checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "        )\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (\n",
    "                all_checkpoint_results[\"checkpoint\"] == ckpt\n",
    "            )\n",
    "\n",
    "            if all_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif all_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "\n",
    "            value = all_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = (\n",
    "                value\n",
    "                - all_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                cur_x,\n",
    "                value,\n",
    "                yerr=value_error,\n",
    "                fmt=\"o\",\n",
    "                color=color,\n",
    "                alpha=0.20,\n",
    "                zorder=0,\n",
    "                markersize=markersize_minor,\n",
    "            )\n",
    "\n",
    "            cur_x += 1\n",
    "\n",
    "        agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "        agg_value = agg_row[metric].item()\n",
    "        agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        ax.errorbar(\n",
    "            agg_loc,\n",
    "            agg_value,\n",
    "            yerr=agg_error,\n",
    "            fmt=\"D\",\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            label=learning_algorithm,\n",
    "            zorder=2,\n",
    "            markersize=markersize_major,\n",
    "            fillstyle=\"none\",\n",
    "        )\n",
    "\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "        custom_lines += [\n",
    "            Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "        ]\n",
    "\n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "\n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "all_k_shots_x = np.stack(\n",
    "    [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    ")\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "\n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "ax.set_title(\" \", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\\nTwitterHateSpeech\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_major)\n",
    "\n",
    "ax.set_ylim(0, 0.25)\n",
    "ax.set_yticks([0.0, 0.05, 0.10, 0.15, 0.20, 0.25])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_gat_transfer.pdf\"\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_transfer = all_checkpoint_results.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"k\"] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"k\"] == 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=all_checkpoint_results[all_checkpoint_results[\"k\"] <= 16],\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "\n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_all_checkpoint_results = all_checkpoint_results.copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Baseline on Twitter Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"subgraphs\": [\"l339inkn\", \"vyeuzmyc\", \"20asxsz3\", \"6046x2gc\", \"pxllyec4\"],\n",
    "    \"maml_lh\": [\"cjvoiuqn\", \"y5zaa74e\", \"k2l9yy52\", \"7rk7bfgn\", \"f9xtwlxp\"],\n",
    "    \"protomaml\": [\"4kd4uk24\", \"ev8f5mch\", \"cj4jv0pl\", \"8qfxh531\", \"m4rg2i2i\"],\n",
    "}\n",
    "\n",
    "all_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"twitterHateSpeech\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        all_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#all_checkpoint_results = all_checkpoint_results[0]\n",
    "all_checkpoint_results = pd.concat(all_checkpoint_results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out some early attempts with too high learning rate\n",
    "# These are noisy data points\n",
    "all_checkpoint_results[(all_checkpoint_results[\"n_updates\"] == 25)] = (\n",
    "    all_checkpoint_results[(all_checkpoint_results[\"n_updates\"] == 25)]\n",
    "    .sort_values(by=[\"learning_algorithm\", \"checkpoint\", \"k\", \"inner_lr\", \"inner_head_lr\"])\n",
    "    .drop_duplicates(subset=[\"learning_algorithm\", \"checkpoint\", \"k\"])\n",
    ")\n",
    "\n",
    "all_checkpoint_results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_all_checkpoint_results = all_checkpoint_results.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=all_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_values = defaultdict(list)\n",
    "custom_lines = []\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "for k in all_k_shots:\n",
    "    matching_k = all_checkpoint_results[\"k\"] == k\n",
    "\n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (\n",
    "            all_checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "        )\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (\n",
    "                all_checkpoint_results[\"checkpoint\"] == ckpt\n",
    "            )\n",
    "\n",
    "            if all_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif all_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "\n",
    "            value = all_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = (\n",
    "                value\n",
    "                - all_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                cur_x,\n",
    "                value,\n",
    "                yerr=value_error,\n",
    "                fmt=\"o\",\n",
    "                color=color,\n",
    "                alpha=0.20,\n",
    "                zorder=0,\n",
    "                markersize=markersize_minor,\n",
    "            )\n",
    "\n",
    "            cur_x += 1\n",
    "\n",
    "        agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "        agg_value = agg_row[metric].item()\n",
    "        agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        ax.errorbar(\n",
    "            agg_loc,\n",
    "            agg_value,\n",
    "            yerr=agg_error,\n",
    "            fmt=\"D\",\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            label=learning_algorithm,\n",
    "            zorder=2,\n",
    "            markersize=markersize_major,\n",
    "            fillstyle=\"none\",\n",
    "        )\n",
    "\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "        custom_lines += [\n",
    "            Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "        ]\n",
    "\n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "\n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=len(all_k_shots) * len(checkpoints), dtype=float)\n",
    "all_k_shots_x = np.stack(\n",
    "    [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    ")\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "    x = np.stack(\n",
    "        [np.ones_like(x_ticks, dtype=float), all_k_shots], axis=1\n",
    "    )\n",
    "    w_prefix = np.linalg.inv(x.T @ x) @ x.T\n",
    "\n",
    "    y = np.array(values)\n",
    "    w_ml = w_prefix @ y\n",
    "\n",
    "    pred_y = all_k_shots_x @ w_ml\n",
    "\n",
    "    ax.plot(all_k_shots_range, pred_y, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.15),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=all_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"mlp\", mlp_all_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"^\" if checkpoint_type == \"gat\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='v', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"Subgraphs\", \"MAML\", \"ProtoMAML\"] + [\"MLP\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.2, 1.1),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=5,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_mlp_ubs_transfer_comparison_mcc.pdf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"macro_aupr\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"mlp\", mlp_all_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"^\" if checkpoint_type == \"gat\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='v', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"Macro-AUPR\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_mlp_ubs_transfer_comparison_aupr_macro.pdf\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IID MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"maml_lh\": [\"nggrixup\", \"e506xerw\", \"iiu8msnu\", \"x502cvg8\", \"xs3e6fgn\"],\n",
    "}\n",
    "\n",
    "metrics = [\n",
    "    \"supp_improvement\",\n",
    "    \"f1_0\",\n",
    "    \"f1_gain_0\",\n",
    "    \"aupr_0\",\n",
    "    \"f1_1\",\n",
    "    \"f1_gain_1\",\n",
    "    \"aupr_1\",\n",
    "    \"f1_2\",\n",
    "    \"f1_gain_2\",\n",
    "    \"aupr_2\",\n",
    "    \"mcc\",\n",
    "    \"f1_gain_macro\",\n",
    "    \"macro_aupr\"\n",
    "]\n",
    "\n",
    "iid_mlp_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"twitterHateSpeech\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        iid_mlp_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#iid_mlp_checkpoint_results = iid_mlp_checkpoint_results[0]\n",
    "iid_mlp_checkpoint_results = pd.concat(iid_mlp_checkpoint_results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out some bad attempts with too high k-shot\n",
    "iid_mlp_checkpoint_results = iid_mlp_checkpoint_results[(iid_mlp_checkpoint_results[\"k\"] <= 16)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=iid_mlp_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_values = defaultdict(list)\n",
    "custom_lines = []\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "for k in all_k_shots:\n",
    "    matching_k = iid_mlp_checkpoint_results[\"k\"] == k\n",
    "\n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (\n",
    "            iid_mlp_checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "        )\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (\n",
    "                iid_mlp_checkpoint_results[\"checkpoint\"] == ckpt\n",
    "            )\n",
    "\n",
    "            if iid_mlp_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif iid_mlp_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "\n",
    "            value = iid_mlp_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = (\n",
    "                value\n",
    "                - iid_mlp_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                cur_x,\n",
    "                value,\n",
    "                yerr=value_error,\n",
    "                fmt=\"o\",\n",
    "                color=color,\n",
    "                alpha=0.20,\n",
    "                zorder=0,\n",
    "                markersize=markersize_minor,\n",
    "            )\n",
    "\n",
    "            cur_x += 1\n",
    "\n",
    "        agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "        agg_value = agg_row[metric].item()\n",
    "        agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        ax.errorbar(\n",
    "            agg_loc,\n",
    "            agg_value,\n",
    "            yerr=agg_error,\n",
    "            fmt=\"D\",\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            label=learning_algorithm,\n",
    "            zorder=2,\n",
    "            markersize=markersize_major,\n",
    "            fillstyle=\"none\",\n",
    "        )\n",
    "\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "        custom_lines += [\n",
    "            Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "        ]\n",
    "\n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "\n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=len(all_k_shots) * len(checkpoints), dtype=float)\n",
    "all_k_shots_x = np.stack(\n",
    "    [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    ")\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "    x = np.stack(\n",
    "        [np.ones_like(x_ticks, dtype=float), all_k_shots], axis=1\n",
    "    )\n",
    "    w_prefix = np.linalg.inv(x.T @ x) @ x.T\n",
    "\n",
    "    y = np.array(values)\n",
    "    w_ml = w_prefix @ y\n",
    "\n",
    "    pred_y = all_k_shots_x @ w_ml\n",
    "\n",
    "    ax.plot(all_k_shots_range, pred_y, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.15),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\", \"f1_gain_0\", \"f1_gain_1\", \"f1_gain_2\", \"macro_aupr\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=iid_mlp_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset GATs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    # These were not trained as MAML_RH\n",
    "    # As these are reset models\n",
    "    # But the fairest comparsion is to MAML_RH\n",
    "    \"maml_rh\": [\"vb49pmtr\", \"8kixi0s8\", \"vzswebea\", \"a4xe91oq\", \"6syunm30\"],\n",
    "    \"protomaml\": [\"9o4wp36l\", \"ouxd7twt\", \"aimkj4sa\", \"euh2mnqo\", \"p33ybhsn\"],\n",
    "}\n",
    "\n",
    "reset_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint_seed, checkpoint in enumerate(ckpts):\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"twitterHateSpeech\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"reset_{checkpoint}_{checkpoint_seed}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        reset_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#reset_checkpoint_results = reset_checkpoint_results[0]\n",
    "reset_checkpoint_results = pd.concat(reset_checkpoint_results).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=reset_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_values = defaultdict(list)\n",
    "custom_lines = []\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "for k in all_k_shots:\n",
    "    matching_k = reset_checkpoint_results[\"k\"] == k\n",
    "\n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (\n",
    "            reset_checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "        )\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (\n",
    "                reset_checkpoint_results[\"checkpoint\"] == ckpt\n",
    "            )\n",
    "\n",
    "            if reset_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif reset_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "\n",
    "            value = reset_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = (\n",
    "                value\n",
    "                - reset_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                cur_x,\n",
    "                value,\n",
    "                yerr=value_error,\n",
    "                fmt=\"o\",\n",
    "                color=color,\n",
    "                alpha=0.20,\n",
    "                zorder=0,\n",
    "                markersize=markersize_minor,\n",
    "            )\n",
    "\n",
    "            cur_x += 1\n",
    "\n",
    "        agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "        agg_value = agg_row[metric].item()\n",
    "        agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        ax.errorbar(\n",
    "            agg_loc,\n",
    "            agg_value,\n",
    "            yerr=agg_error,\n",
    "            fmt=\"D\",\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            label=learning_algorithm,\n",
    "            zorder=2,\n",
    "            markersize=markersize_major,\n",
    "            fillstyle=\"none\",\n",
    "        )\n",
    "\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "        custom_lines += [\n",
    "            Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "        ]\n",
    "\n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "\n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=len(all_k_shots) * len(checkpoints), dtype=float)\n",
    "all_k_shots_x = np.stack(\n",
    "    [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    ")\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "\n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.15),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=reset_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_reset\", reset_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"<\" if checkpoint_type == \"gat_reset\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='<', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"MAML\", \"ProtoMAML\"] + [\"Reset\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=4,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_random_transfer_comparison_mcc.pdf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"macro_aupr\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_reset\", reset_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"<\" if checkpoint_type == \"gat_reset\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='<', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"Macro-AUPR\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_random_transfer_comparison_macro_aupr.pdf\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATs + User initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"subgraphs\": [\"vb49pmtr\", \"8kixi0s8\", \"vzswebea\", \"a4xe91oq\", \"6syunm30\"],\n",
    "    \"maml_lh\": [\"zqhx6x3b\", \"11pt2nis\", \"ruy4hp9o\", \"nlfyh80j\", \"06pfaw4f\"],\n",
    "    \"maml_rh\": [\"rpu3r1rm\", \"35neqj40\", \"xycl2xbl\", \"9hdrq4an\", \"905yf717\"],\n",
    "    \"prototypical\": [\"yjnx3e9w\", \"5e0vvr04\", \"br7qcerq\", \"6c8tvtwn\", \"ahp19u65\"],\n",
    "    \"protomaml\": [\"9o4wp36l\", \"ouxd7twt\", \"aimkj4sa\", \"euh2mnqo\", \"p33ybhsn\"],\n",
    "}\n",
    "\n",
    "user_init_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"twitterHateSpeech\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}_avg_pool_user_init\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "        \n",
    "        print(checkpoint_results_weighted.shape[0])\n",
    "\n",
    "        user_init_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#user_init_checkpoint_results = user_init_checkpoint_results[0]\n",
    "user_init_checkpoint_results = pd.concat(user_init_checkpoint_results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_init_checkpoint_results[(user_init_checkpoint_results[\"checkpoint\"] == \"9o4wp36l\") & (user_init_checkpoint_results[\"k\"] == 16)][\"mcc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=user_init_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=user_init_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_values = defaultdict(list)\n",
    "custom_lines = []\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "for k in all_k_shots:\n",
    "    matching_k = user_init_checkpoint_results[\"k\"] == k\n",
    "\n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (\n",
    "            user_init_checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "        )\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (\n",
    "                user_init_checkpoint_results[\"checkpoint\"] == ckpt\n",
    "            )\n",
    "\n",
    "            if user_init_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif user_init_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "\n",
    "            value = user_init_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = (\n",
    "                value\n",
    "                - user_init_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                cur_x,\n",
    "                value,\n",
    "                yerr=value_error,\n",
    "                fmt=\"o\",\n",
    "                color=color,\n",
    "                alpha=0.20,\n",
    "                zorder=0,\n",
    "                markersize=markersize_minor,\n",
    "            )\n",
    "\n",
    "            cur_x += 1\n",
    "\n",
    "        agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "        agg_value = agg_row[metric].item()\n",
    "        agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        ax.errorbar(\n",
    "            agg_loc,\n",
    "            agg_value,\n",
    "            yerr=agg_error,\n",
    "            fmt=\"D\",\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            label=learning_algorithm,\n",
    "            zorder=2,\n",
    "            markersize=markersize_major,\n",
    "            fillstyle=\"none\",\n",
    "        )\n",
    "\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "        custom_lines += [\n",
    "            Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "        ]\n",
    "\n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "\n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=len(all_k_shots) * len(checkpoints), dtype=float)\n",
    "all_k_shots_x = np.stack(\n",
    "    [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    ")\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "\n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.15),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_user\", user_init_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='>', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='v', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"Subgraphs\", \"MAML-LH\", \"MAML-RH\", \"ProtoNet\", \"ProtoMAML\"] + [\"User\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=7,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_gat_with_user_init_tranfer_mcc.pdf\"\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"macro_aupr\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_user\", user_init_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='>', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='v', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"Macro-AUPR\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_gat_with_user_init_tranfer_macro_aupr.pdf\"\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_user_init_results = user_init_checkpoint_results.copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme $k$-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "tested_learning_algorithms = [\n",
    "    \"prototypical\"\n",
    "]\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16, 32, 64, 128, 256]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_user\", user_init_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    aggregated_df = aggregated_df.loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "    \n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            \n",
    "            if learning_algorithm not in tested_learning_algorithms:\n",
    "                continue\n",
    "            \n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "learning_algorithm = \"prototypical\"\n",
    "    \n",
    "color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='>', markerfacecolor='k', color='w', lw=0, markersize=markersize_major),\n",
    "    Line2D([0], [0], marker='v', markerfacecolor='k', color='w', lw=0, markersize=markersize_major),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"ProtoNet\"] + [\"User\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=10,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_extreme_kshot_mcc.pdf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "tested_learning_algorithms = [\n",
    "    \"prototypical\"\n",
    "]\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16, 32, 64, 128, 256]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat\", twitter_transfer)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    aggregated_df = aggregated_df.loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "    \n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            \n",
    "            if learning_algorithm not in tested_learning_algorithms:\n",
    "                continue\n",
    "            \n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "learning_algorithm = \"prototypical\"\n",
    "    \n",
    "color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_extreme_kshot_macro_aupr.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=all_checkpoint_results[all_checkpoint_results[\"learning_algorithm\"] == \"prototypical\"],\n",
    "        by=[\"k\", \"learning_algorithm\", \"user_init\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"f1_0\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "tested_learning_algorithms = [\n",
    "    \"prototypical\"\n",
    "]\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16, 32, 64, 128, 256]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_user\", user_init_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    aggregated_df = aggregated_df.loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "    \n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            \n",
    "            if learning_algorithm not in tested_learning_algorithms:\n",
    "                continue\n",
    "            \n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='>', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"F1-0\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()) + [\"User\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=4,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"f1_1\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "tested_learning_algorithms = [\n",
    "    \"prototypical\"\n",
    "]\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16, 32, 64, 128, 256]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_user\", user_init_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    aggregated_df = aggregated_df.loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "    \n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            \n",
    "            if learning_algorithm not in tested_learning_algorithms:\n",
    "                continue\n",
    "            \n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='>', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"F1-1\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()) + [\"User\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=4,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"f1_2\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "tested_learning_algorithms = [\n",
    "    \"prototypical\"\n",
    "]\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16, 32, 64, 128, 256]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_user\", user_init_checkpoint_results), (\"gat\", gat_all_checkpoint_results)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    aggregated_df = aggregated_df.loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "    \n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            \n",
    "            if learning_algorithm not in tested_learning_algorithms:\n",
    "                continue\n",
    "            \n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\">\" if checkpoint_type == \"gat_user\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='>', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"F1-2\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()) + [\"User\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=10,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_all_checkpoint_results[(gat_all_checkpoint_results[\"user_init\"] == False) & (gat_all_checkpoint_results[\"k\"] == 256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=gat_all_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\", \"user_init\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "\n",
    "    all_agg_dfs[-1] = all_agg_dfs[-1].loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "\n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\",\n",
    "    ).reset_index()\n",
    "\n",
    "user_init_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"f1_2\", \"aupr_2\", \"mcc\"]:\n",
    "    user_init_agg_dfs += [normalize_by_group(\n",
    "        df=user_init_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\", \"user_init\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "\n",
    "    user_init_agg_dfs[-1] = user_init_agg_dfs[-1].loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "\n",
    "user_init_agg_dfs = pd.concat(\n",
    "    user_init_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\",\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs = pd.merge(all_agg_dfs, user_init_agg_dfs, how='outer')\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=tested_learning_algorithms,\n",
    "    ordered=True,\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\", \"user_init\"]).set_index(keys=[\"k\", \"learning_algorithm\", \"user_init\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HealthStory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"subgraphs\": [\"vb49pmtr\", \"8kixi0s8\", \"vzswebea\", \"a4xe91oq\", \"6syunm30\"],\n",
    "    \"maml_lh\": [\"zqhx6x3b\", \"11pt2nis\", \"ruy4hp9o\", \"nlfyh80j\", \"06pfaw4f\"],\n",
    "    \"maml_rh\": [\"rpu3r1rm\", \"35neqj40\", \"xycl2xbl\", \"9hdrq4an\", \"905yf717\"],\n",
    "    \"prototypical\": [\"2crszon2\", \"wdhcjrmp\", \"zwwyk83h\", \"lku2gxb8\", \"4pj3ewdu\"],\n",
    "    \"protomaml\": [\"9o4wp36l\", \"ouxd7twt\", \"aimkj4sa\", \"euh2mnqo\", \"p33ybhsn\"],\n",
    "}\n",
    "\n",
    "metrics = [\n",
    "    \"supp_improvement\",\n",
    "    \"f1_0\",\n",
    "    \"f1_1\",\n",
    "    \"f1_2\",\n",
    "    \"f1_gain_0\",\n",
    "    \"f1_gain_1\",\n",
    "    \"f1_gain_2\",\n",
    "    \"f1_gain_macro\",\n",
    "    \"macro_auprg\",\n",
    "    \"mcc\",\n",
    "]\n",
    "\n",
    "all_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"HealthStory\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        all_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#all_checkpoint_results = all_checkpoint_results[0]\n",
    "all_checkpoint_results = pd.concat(all_checkpoint_results).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_checkpoint_results = all_checkpoint_results[all_checkpoint_results[\"n_updates\"] == 25]\n",
    "all_checkpoint_results = all_checkpoint_results.sort_values(by=[\"k\", \"learning_algorithm\", \"inner_lr\", \"n_updates\",])\n",
    "\n",
    "all_checkpoint_results = all_checkpoint_results.drop_duplicates(subset=[\"k\", \"learning_algorithm\", \"checkpoint\", \"n_updates\",], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoint_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_transfer_baseline = all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0)]\n",
    "zero_shot_transfer_baseline = zero_shot_transfer_baseline[zero_shot_transfer_baseline[\"k\"] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_adaptation_checkpoints = all_checkpoint_results[~((all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0))]\n",
    "\n",
    "low_adaptation_checkpoints = low_adaptation_checkpoints.sort_values(by=[\"k\", \"learning_algorithm\", \"n_updates\", \"inner_lr\",])\n",
    "\n",
    "low_adaptation_checkpoints = low_adaptation_checkpoints.drop_duplicates(subset=[\"k\", \"learning_algorithm\", \"checkpoint\",], keep=\"first\")\n",
    "\n",
    "low_adaptation_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.03209, 9.72632/4))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_locs = defaultdict(list)\n",
    "agg_values = defaultdict(list)\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "\n",
    "zero_shot_vals = zero_shot_transfer_baseline[metric].to_numpy()\n",
    "zero_shot_error = zero_shot_vals - zero_shot_transfer_baseline[f\"{lower_metric}_lb\"].to_numpy()\n",
    "\n",
    "ax.errorbar(\n",
    "    x=np.arange(cur_x, cur_x + zero_shot_vals.shape[0]),\n",
    "    y=zero_shot_vals,\n",
    "    yerr=zero_shot_error,\n",
    "    fmt='o',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=0.20,\n",
    "    zorder=1\n",
    "    )\n",
    "\n",
    "ax.errorbar(\n",
    "    np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean(),\n",
    "    zero_shot_vals.mean(),\n",
    "    fmt='D',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=1.0,\n",
    "    label=learning_algorithm,\n",
    "    zorder=2\n",
    "    )\n",
    "\n",
    "ax.hlines(\n",
    "    y=zero_shot_vals.mean(), \n",
    "    xmin=0,\n",
    "    xmax=1000,\n",
    "    colors=[\"gray\"],\n",
    "    zorder=1,\n",
    "    linestyles=\"--\",\n",
    "    alpha=0.75,\n",
    "    )\n",
    "\n",
    "x_ticks += [np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean()]\n",
    "x_tick_labels += [\"0\"]\n",
    "\n",
    "cur_x = zero_shot_vals.shape[0] + 35\n",
    "\n",
    "for k in all_k_shots:\n",
    "    matching_k = low_adaptation_checkpoints[\"k\"] == k\n",
    "    \n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (low_adaptation_checkpoints[\"learning_algorithm\"] == learning_algorithm)\n",
    "        \n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (low_adaptation_checkpoints[\"checkpoint\"] == ckpt)\n",
    "            \n",
    "            if low_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif low_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "                        \n",
    "            value = low_adaptation_checkpoints[matching_ckpt][lower_metric].item()\n",
    "            value_error = value - low_adaptation_checkpoints[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            \n",
    "            ax.errorbar(cur_x, value, yerr=value_error, fmt='o', color=cmap(i), alpha=0.20, zorder=1)\n",
    "            \n",
    "            cur_x += 1\n",
    "\n",
    "        agg_value = low_adaptation_checkpoints[matching_learning_alg][lower_metric].mean()#.item()\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        agg_locs[learning_algorithm] += [agg_loc]\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "        \n",
    "        ax.errorbar(agg_loc, agg_value, fmt='D', color=cmap(i), alpha=1.0, label=learning_algorithm, zorder=2)\n",
    "        \n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "    \n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xlim(-25, cur_x)\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cmap(0), lw=4),\n",
    "    Line2D([0], [0], color=cmap(1), lw=4),\n",
    "    Line2D([0], [0], color=cmap(2), lw=4),\n",
    "    Line2D([0], [0], color=cmap(3), lw=4),\n",
    "    Line2D([0], [0], color=cmap(4), lw=4),\n",
    "    ]\n",
    "\n",
    "#ax.legend(custom_lines, list(checkpoints.keys()))\n",
    "\n",
    "all_k_shots_range = np.arange(-10, cur_x+10, step=cur_x//10, dtype=float)\n",
    "all_k_shots_x = np.stack([np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1)\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(agg_locs[learning_algorithm], dtype=float)\n",
    "    x = np.stack([np.ones_like(agg_locs[learning_algorithm], dtype=float), all_k_shots], axis=1)\n",
    "    w_prefix = np.linalg.inv(x.T @ x) @ x.T\n",
    "    \n",
    "    y = np.array(values)\n",
    "    w_ml = w_prefix @ y\n",
    "    \n",
    "    pred_y = all_k_shots_x @ w_ml\n",
    "    \n",
    "    ax.plot(all_k_shots_range, pred_y, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "ax.set_title(\"HealthStory\\n(low adaptation)\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=9)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_transfer_baseline = all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0)]\n",
    "zero_shot_transfer_baseline = zero_shot_transfer_baseline[zero_shot_transfer_baseline[\"k\"] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_adaptation_checkpoints = all_checkpoint_results[~((all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0))]\n",
    "\n",
    "high_adaptation_checkpoints = high_adaptation_checkpoints.sort_values(by=[\"k\", \"learning_algorithm\", \"n_updates\", \"inner_lr\",])\n",
    "\n",
    "high_adaptation_checkpoints = high_adaptation_checkpoints.drop_duplicates(subset=[\"k\", \"learning_algorithm\", \"checkpoint\",], keep=\"last\")\n",
    "\n",
    "high_adaptation_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.03209, 9.72632/4))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_locs = defaultdict(list)\n",
    "agg_values = defaultdict(list)\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "\n",
    "zero_shot_vals = zero_shot_transfer_baseline[metric].to_numpy()\n",
    "zero_shot_error = zero_shot_vals - zero_shot_transfer_baseline[f\"{lower_metric}_lb\"].to_numpy()\n",
    "\n",
    "ax.errorbar(\n",
    "    x=np.arange(cur_x, cur_x + zero_shot_vals.shape[0]),\n",
    "    y=zero_shot_vals,\n",
    "    yerr=zero_shot_error,\n",
    "    fmt='o',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=0.20,\n",
    "    zorder=1\n",
    "    )\n",
    "\n",
    "ax.errorbar(\n",
    "    np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean(),\n",
    "    zero_shot_vals.mean(),\n",
    "    fmt='D',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=1.0,\n",
    "    label=learning_algorithm,\n",
    "    zorder=2\n",
    "    )\n",
    "\n",
    "ax.hlines(\n",
    "    y=zero_shot_vals.mean(), \n",
    "    xmin=0,\n",
    "    xmax=1000,\n",
    "    colors=[\"gray\"],\n",
    "    zorder=1,\n",
    "    linestyles=\"--\",\n",
    "    alpha=0.75,\n",
    "    )\n",
    "\n",
    "x_ticks += [np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean()]\n",
    "x_tick_labels += [\"0\"]\n",
    "\n",
    "cur_x = zero_shot_vals.shape[0] + 35\n",
    "\n",
    "for k in all_k_shots:\n",
    "    matching_k = high_adaptation_checkpoints[\"k\"] == k\n",
    "    \n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (high_adaptation_checkpoints[\"learning_algorithm\"] == learning_algorithm)\n",
    "        \n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (high_adaptation_checkpoints[\"checkpoint\"] == ckpt)\n",
    "            \n",
    "            if high_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif high_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "                        \n",
    "            value = high_adaptation_checkpoints[matching_ckpt][lower_metric].item()\n",
    "            value_error = value - high_adaptation_checkpoints[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            \n",
    "            ax.errorbar(cur_x, value, yerr=value_error, fmt='o', color=cmap(i), alpha=0.20, zorder=1)\n",
    "            \n",
    "            cur_x += 1\n",
    "\n",
    "        agg_value = high_adaptation_checkpoints[matching_learning_alg][lower_metric].mean()#.item()\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        agg_locs[learning_algorithm] += [agg_loc]\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "        \n",
    "        ax.errorbar(agg_loc, agg_value, fmt='D', color=cmap(i), alpha=1.0, label=learning_algorithm, zorder=2)\n",
    "        \n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "    \n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xlim(-25, cur_x)\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cmap(0), lw=4),\n",
    "    Line2D([0], [0], color=cmap(1), lw=4),\n",
    "    Line2D([0], [0], color=cmap(2), lw=4),\n",
    "    Line2D([0], [0], color=cmap(3), lw=4),\n",
    "    Line2D([0], [0], color=cmap(4), lw=4),\n",
    "    ]\n",
    "\n",
    "#ax.legend(custom_lines, list(checkpoints.keys()))\n",
    "\n",
    "all_k_shots_range = np.arange(-10, cur_x+10, step=cur_x//10, dtype=float)\n",
    "all_k_shots_x = np.stack([np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1)\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(agg_locs[learning_algorithm], dtype=float)\n",
    "    x = np.stack([np.ones_like(agg_locs[learning_algorithm], dtype=float), all_k_shots], axis=1)\n",
    "    w_prefix = np.linalg.inv(x.T @ x) @ x.T\n",
    "    \n",
    "    y = np.array(values)\n",
    "    w_ml = w_prefix @ y\n",
    "    \n",
    "    pred_y = all_k_shots_x @ w_ml\n",
    "    \n",
    "    ax.plot(all_k_shots_range, pred_y, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "ax.set_title(\"HealthStory\\n(high adaptation)\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=9)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"subgraphs\": [\"vb49pmtr\", \"8kixi0s8\", \"vzswebea\", \"a4xe91oq\", \"6syunm30\"],\n",
    "    \"maml_lh\": [\"zqhx6x3b\", \"11pt2nis\", \"ruy4hp9o\", \"nlfyh80j\", \"06pfaw4f\"],\n",
    "    \"maml_rh\": [\"rpu3r1rm\", \"35neqj40\", \"xycl2xbl\", \"9hdrq4an\", \"905yf717\"],\n",
    "    \"prototypical\": [\"2crszon2\", \"wdhcjrmp\", \"zwwyk83h\", \"lku2gxb8\", \"4pj3ewdu\"],\n",
    "    \"prototypical\": [\"yjnx3e9w\", \"5e0vvr04\", \"br7qcerq\", \"6c8tvtwn\", \"ahp19u65\"],\n",
    "    \"protomaml\": [\"9o4wp36l\", \"ouxd7twt\", \"aimkj4sa\", \"euh2mnqo\", \"p33ybhsn\"],\n",
    "}\n",
    "\n",
    "metrics = [\n",
    "    \"supp_improvement\",\n",
    "    \"f1_0\",\n",
    "    \"aupr_0\",\n",
    "    \"f1_1\",\n",
    "    \"aupr_1\",\n",
    "    \"mcc\",\n",
    "]\n",
    "\n",
    "all_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"CoAID\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        all_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#all_checkpoint_results = all_checkpoint_results[0]\n",
    "all_checkpoint_results = pd.concat(all_checkpoint_results).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoint_results[all_checkpoint_results[\"learning_algorithm\"] == \"protomaml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out some early attempts with too high learning rate\n",
    "# These are noisy data points\n",
    "all_checkpoint_results = all_checkpoint_results[~((all_checkpoint_results.class_weights == (1.0, 20.0)) | (all_checkpoint_results.class_weights == (1.0, 3.0)))]\n",
    "\n",
    "all_checkpoint_results[(all_checkpoint_results[\"n_updates\"] == 25)] = (\n",
    "    all_checkpoint_results[(all_checkpoint_results[\"n_updates\"] == 25)]\n",
    "    .sort_values(by=[\"learning_algorithm\", \"checkpoint\", \"k\", \"inner_lr\", \"inner_head_lr\"])\n",
    "    .drop_duplicates(subset=[\"learning_algorithm\", \"checkpoint\", \"k\"])\n",
    ")\n",
    "\n",
    "all_checkpoint_results.dropna(inplace=True)\n",
    "\n",
    "all_checkpoint_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_transfer_baseline = all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0)]\n",
    "zero_shot_transfer_baseline = zero_shot_transfer_baseline[zero_shot_transfer_baseline[\"k\"] == 4]\n",
    "\n",
    "low_adaptation_checkpoints = pd.concat([zero_shot_transfer_baseline, all_checkpoint_results]).drop_duplicates(subset=[\"k\", \"n_updates\", \"inner_lr\", \"inner_head_lr\", \"checkpoint\"], keep=False)\n",
    "low_adaptation_checkpoints = low_adaptation_checkpoints.sort_values(by=[\"k\", \"learning_algorithm\", \"n_updates\", \"inner_lr\",])\n",
    "low_adaptation_checkpoints = low_adaptation_checkpoints.drop_duplicates(subset=[\"k\", \"learning_algorithm\", \"checkpoint\",], keep=\"first\")\n",
    "\n",
    "zero_shot_transfer_baseline[\"k\"] = 0\n",
    "low_adaptation_checkpoints = pd.concat([zero_shot_transfer_baseline, low_adaptation_checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "metric = \"aupr_1\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=low_adaptation_checkpoints,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "learning_algorithms_with_values = set(aggregated_df.index.get_level_values(level=1).unique().to_list())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_locs = defaultdict(list)\n",
    "agg_values = defaultdict(list)\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "\n",
    "zero_shot_vals = zero_shot_transfer_baseline[metric].to_numpy()\n",
    "zero_shot_error = zero_shot_vals - zero_shot_transfer_baseline[f\"{lower_metric}_lb\"].to_numpy()\n",
    "\n",
    "ax.errorbar(\n",
    "    x=np.arange(cur_x, cur_x + zero_shot_vals.shape[0]),\n",
    "    y=zero_shot_vals,\n",
    "    yerr=zero_shot_error,\n",
    "    fmt='o',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=0.20,\n",
    "    zorder=1,\n",
    "    markersize=markersize_minor,\n",
    "    )\n",
    "\n",
    "ax.errorbar(\n",
    "    np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean(),\n",
    "    aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}\"],\n",
    "    yerr=aggregated_df.xs((0.0, \"subgraphs\"))[f\"{metric}_ub\"]-aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}\"],\n",
    "    fmt='D',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=1.0,\n",
    "    label=learning_algorithm,\n",
    "    zorder=2,\n",
    "    markersize=markersize_major,\n",
    "    fillstyle=\"none\",\n",
    "    )\n",
    "\n",
    "ax.hlines(\n",
    "    y=zero_shot_vals.mean(), \n",
    "    xmin=0,\n",
    "    xmax=1000,\n",
    "    colors=[\"gray\"],\n",
    "    zorder=1,\n",
    "    linestyles=\"--\",\n",
    "    alpha=0.75,\n",
    "    )\n",
    "\n",
    "x_ticks += [np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean()]\n",
    "x_tick_labels += [\"0\"]\n",
    "\n",
    "cur_x = zero_shot_vals.shape[0] + 35\n",
    "\n",
    "for k in all_k_shots:\n",
    "    matching_k = low_adaptation_checkpoints[\"k\"] == k\n",
    "    \n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        \n",
    "        if learning_algorithm not in learning_algorithms_with_values:\n",
    "            continue\n",
    "        \n",
    "        matching_learning_alg = matching_k & (low_adaptation_checkpoints[\"learning_algorithm\"] == learning_algorithm)\n",
    "        \n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (low_adaptation_checkpoints[\"checkpoint\"] == ckpt)\n",
    "            \n",
    "            if low_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif low_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "                        \n",
    "            value = low_adaptation_checkpoints[matching_ckpt][lower_metric].item()\n",
    "            value_error = value - low_adaptation_checkpoints[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            \n",
    "            ax.errorbar(cur_x, value, yerr=value_error, fmt='o', color=cmap(i), alpha=0.20, zorder=1, markersize=markersize_minor)\n",
    "            \n",
    "            cur_x += 1\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "        agg_value = aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "        agg_error = aggregated_df.xs((k, learning_algorithm))[f\"{lower_metric}_ub\"] - aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "\n",
    "        agg_locs[learning_algorithm] += [agg_loc]\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "        \n",
    "        ax.errorbar(agg_loc, agg_value, yerr=agg_error, fmt='D', color=cmap(i), alpha=1.0, label=learning_algorithm, zorder=2, markersize=markersize_major, fillstyle=\"none\",)\n",
    "        \n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "    \n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xlim(-25, cur_x)\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cmap(0), lw=4),\n",
    "    Line2D([0], [0], color=cmap(1), lw=4),\n",
    "    Line2D([0], [0], color=cmap(2), lw=4),\n",
    "    Line2D([0], [0], color=cmap(3), lw=4),\n",
    "    Line2D([0], [0], color=cmap(4), lw=4),\n",
    "    ]\n",
    "\n",
    "#ax.legend(custom_lines, list(checkpoints.keys()))\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks[1:]), max(x_ticks[1:]), step=1, dtype=float)\n",
    "all_k_shots_x = np.stack([np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1)\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks[1:], dtype=float)\n",
    "    \n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"HealthStory\\n(low adaptation)\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\\nLow Adaptation\", fontsize=fontsize_major)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=low_adaptation_checkpoints,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_transfer_baseline = all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0)]\n",
    "zero_shot_transfer_baseline = zero_shot_transfer_baseline[zero_shot_transfer_baseline[\"k\"] == 4]\n",
    "\n",
    "high_adaptation_checkpoints = pd.concat([zero_shot_transfer_baseline, all_checkpoint_results]).drop_duplicates(subset=[\"k\", \"n_updates\", \"inner_lr\", \"inner_head_lr\", \"checkpoint\"], keep=False)\n",
    "high_adaptation_checkpoints = high_adaptation_checkpoints.sort_values(by=[\"k\", \"learning_algorithm\", \"n_updates\", \"inner_lr\",])\n",
    "high_adaptation_checkpoints = high_adaptation_checkpoints.drop_duplicates(subset=[\"k\", \"learning_algorithm\", \"checkpoint\",], keep=\"last\")\n",
    "\n",
    "zero_shot_transfer_baseline[\"k\"] = 0\n",
    "high_adaptation_checkpoints = pd.concat([zero_shot_transfer_baseline, high_adaptation_checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"protomaml\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_adaptation_checkpoints.learning_algorithm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=high_adaptation_checkpoints,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "learning_algorithms_with_values = set(aggregated_df.index.get_level_values(level=1).unique().to_list())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_locs = defaultdict(list)\n",
    "agg_values = defaultdict(list)\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "\n",
    "zero_shot_vals = zero_shot_transfer_baseline[metric].to_numpy()\n",
    "zero_shot_error = zero_shot_vals - zero_shot_transfer_baseline[f\"{lower_metric}_lb\"].to_numpy()\n",
    "\n",
    "ax.errorbar(\n",
    "    x=np.arange(cur_x, cur_x + zero_shot_vals.shape[0]),\n",
    "    y=zero_shot_vals,\n",
    "    yerr=zero_shot_error,\n",
    "    fmt='o',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=0.20,\n",
    "    zorder=1,\n",
    "    markersize=markersize_minor,\n",
    "    )\n",
    "\n",
    "ax.errorbar(\n",
    "    np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean(),\n",
    "    aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}\"],\n",
    "    yerr=aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}_ub\"]-aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}\"],\n",
    "    fmt='D',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=1.0,\n",
    "    label=learning_algorithm,\n",
    "    zorder=2,\n",
    "    markersize=markersize_major,\n",
    "    fillstyle=\"none\",\n",
    "    )\n",
    "\n",
    "ax.hlines(\n",
    "    y=zero_shot_vals.mean(), \n",
    "    xmin=0,\n",
    "    xmax=1000,\n",
    "    colors=[\"gray\"],\n",
    "    zorder=1,\n",
    "    linestyles=\"--\",\n",
    "    alpha=0.75,\n",
    "    )\n",
    "\n",
    "x_ticks += [np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean()]\n",
    "x_tick_labels += [\"0\"]\n",
    "\n",
    "cur_x = zero_shot_vals.shape[0] + 35\n",
    "\n",
    "for k in all_k_shots:\n",
    "    matching_k = high_adaptation_checkpoints[\"k\"] == k\n",
    "    \n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        \n",
    "        if learning_algorithm not in learning_algorithms_with_values:\n",
    "            continue\n",
    "        \n",
    "        matching_learning_alg = matching_k & (high_adaptation_checkpoints[\"learning_algorithm\"] == learning_algorithm)\n",
    "        \n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (high_adaptation_checkpoints[\"checkpoint\"] == ckpt)\n",
    "            \n",
    "            if high_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif high_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "                        \n",
    "            value = high_adaptation_checkpoints[matching_ckpt][lower_metric].item()\n",
    "            value_error = value - high_adaptation_checkpoints[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            \n",
    "            ax.errorbar(cur_x, value, yerr=value_error, fmt='o', color=cmap(i), alpha=0.20, zorder=1, markersize=markersize_minor)\n",
    "            \n",
    "            cur_x += 1\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "        agg_value = aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "        agg_error = aggregated_df.xs((k, learning_algorithm))[f\"{metric}_ub\"] - aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "\n",
    "        agg_locs[learning_algorithm] += [agg_loc]\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "        \n",
    "        ax.errorbar(agg_loc, agg_value, yerr=agg_error, fmt='D', color=cmap(i), alpha=1.0, label=learning_algorithm, zorder=2, markersize=markersize_major, fillstyle=\"none\",)\n",
    "        \n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "    \n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xlim(-25, cur_x)\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cmap(0), lw=0, marker=\"D\", markersize=2),\n",
    "    Line2D([0], [0], color=cmap(1), lw=0, marker=\"D\", markersize=2),\n",
    "    Line2D([0], [0], color=cmap(2), lw=0, marker=\"D\", markersize=2),\n",
    "    Line2D([0], [0], color=cmap(3), lw=0, marker=\"D\", markersize=2),\n",
    "    Line2D([0], [0], color=cmap(4), lw=0, marker=\"D\", markersize=2),\n",
    "    ]\n",
    "\n",
    "#ax.legend(custom_lines, list(checkpoints.keys()))\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks[1:]), max(x_ticks[1:]), step=1, dtype=float)\n",
    "all_k_shots_x = np.stack([np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1)\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks[1:], dtype=float)\n",
    "    \n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "ax.set_title(\" \", fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\\nCoAID\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_major)\n",
    "\n",
    "ax.set_ylim(0, 0.25)\n",
    "ax.set_yticks([0.0, 0.05, 0.10, 0.15, 0.20, 0.25])\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"Subgraphs\", \"MAML-LH\", \"MAML-RH\", \"ProtoNet\", \"ProtoMAML\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/coaid_gat_transfer_high_adapt.pdf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaid_transfer = high_adaptation_checkpoints.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=high_adaptation_checkpoints,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_all_checkpoint_results = high_adaptation_checkpoints.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"subgraphs\": [\"l339inkn\", \"vyeuzmyc\", \"20asxsz3\", \"6046x2gc\", \"pxllyec4\"],\n",
    "    \"maml_lh\": [\"cjvoiuqn\", \"y5zaa74e\", \"k2l9yy52\", \"7rk7bfgn\", \"f9xtwlxp\"],\n",
    "    \"protomaml\": [\"4kd4uk24\", \"ev8f5mch\", \"cj4jv0pl\", \"8qfxh531\", \"m4rg2i2i\"],\n",
    "}\n",
    "\n",
    "all_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint in ckpts:\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"CoAID\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"transfer_{checkpoint}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        all_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#all_checkpoint_results = all_checkpoint_results[0]\n",
    "all_checkpoint_results = pd.concat(all_checkpoint_results).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoint_results = all_checkpoint_results[all_checkpoint_results[\"class_weights\"] == (1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=all_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "learning_algorithms_with_values = set(aggregated_df.index.get_level_values(level=1).unique().to_list())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_locs = defaultdict(list)\n",
    "agg_values = defaultdict(list)\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "\n",
    "for k in all_k_shots:\n",
    "    matching_k = all_checkpoint_results[\"k\"] == k\n",
    "    \n",
    "    for learning_algorithm, ckpts in checkpoints.items():\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        if learning_algorithm not in learning_algorithms_with_values:\n",
    "            continue\n",
    "        \n",
    "        matching_learning_alg = matching_k & (all_checkpoint_results[\"learning_algorithm\"] == learning_algorithm)\n",
    "        \n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (all_checkpoint_results[\"checkpoint\"] == ckpt)\n",
    "            \n",
    "            if all_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif all_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "                        \n",
    "            value = all_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = value - all_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            \n",
    "            ax.errorbar(cur_x, value, yerr=value_error, fmt='o', color=color, alpha=0.20, zorder=1, markersize=markersize_minor)\n",
    "            \n",
    "            cur_x += 1\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "        agg_value = aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "        agg_error = aggregated_df.xs((k, learning_algorithm))[f\"{metric}_ub\"] - aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "\n",
    "        agg_locs[learning_algorithm] += [agg_loc]\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "        \n",
    "        ax.errorbar(agg_loc, agg_value, yerr=agg_error, fmt='D', color=color, alpha=1.0, label=learning_algorithm, zorder=2, markersize=markersize_major, fillstyle=\"none\",)\n",
    "        \n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "    \n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xlim(-25, cur_x)\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cmap(0), lw=4),\n",
    "    Line2D([0], [0], color=cmap(1), lw=4),\n",
    "    Line2D([0], [0], color=cmap(2), lw=4),\n",
    "    Line2D([0], [0], color=cmap(3), lw=4),\n",
    "    Line2D([0], [0], color=cmap(4), lw=4),\n",
    "    ]\n",
    "\n",
    "#ax.legend(custom_lines, list(checkpoints.keys()))\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "all_k_shots_x = np.stack([np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1)\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "    x = np.stack([np.ones_like(x_ticks, dtype=float), all_k_shots], axis=1)\n",
    "    w_prefix = np.linalg.inv(x.T @ x) @ x.T\n",
    "    \n",
    "    y = np.array(values)\n",
    "    w_ml = w_prefix @ y\n",
    "    \n",
    "    pred_y = all_k_shots_x @ w_ml\n",
    "    \n",
    "    ax.plot(all_k_shots_range, pred_y, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"HealthStory\\n(low adaptation)\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\\nHigh Adaptation\", fontsize=fontsize_major)\n",
    "ax.set_ylim(-0.03, 0.27)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_all_checkpoint_results = all_checkpoint_results.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=all_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"MLP UBS\", mlp_all_checkpoint_results), (\"GAT\", high_adaptation_checkpoints)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"^\" if checkpoint_type == \"MLP UBS\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"GAT\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='v', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"Subgraphs\", \"MAML\", \"ProtoMAML\"] + [\"MLP\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.35),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=5,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/coaid_mlp_ubs_transfer_comparison_mcc.pdf\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"aupr_1\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"MLP UBS\", mlp_all_checkpoint_results), (\"GAT\", high_adaptation_checkpoints)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"^\" if checkpoint_type == \"MLP UBS\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"GAT\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='v', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=11)\n",
    "ax.set_ylabel(\"AUPR-Fake\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/coaid_mlp_ubs_transfer_comparison_aupr_1.pdf\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_transfer_baseline = all_checkpoint_results[(all_checkpoint_results[\"learning_algorithm\"] == \"subgraphs\") & (all_checkpoint_results[\"inner_lr\"] == 0.0)]\n",
    "zero_shot_transfer_baseline = zero_shot_transfer_baseline[zero_shot_transfer_baseline[\"k\"] == 4]\n",
    "\n",
    "low_adaptation_checkpoints = pd.concat([zero_shot_transfer_baseline, all_checkpoint_results]).drop_duplicates(subset=[\"k\", \"n_updates\", \"inner_lr\", \"inner_head_lr\", \"checkpoint\"], keep=False)\n",
    "low_adaptation_checkpoints = low_adaptation_checkpoints.sort_values(by=[\"k\", \"learning_algorithm\", \"n_updates\", \"inner_lr\",])\n",
    "low_adaptation_checkpoints = low_adaptation_checkpoints.drop_duplicates(subset=[\"k\", \"learning_algorithm\", \"checkpoint\",], keep=\"first\")\n",
    "\n",
    "zero_shot_transfer_baseline[\"k\"] = 0\n",
    "low_adaptation_checkpoints = pd.concat([zero_shot_transfer_baseline, low_adaptation_checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "metric = \"aupr_1\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=high_adaptation_checkpoints,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "learning_algorithms_with_values = set(aggregated_df.index.get_level_values(level=1).unique().to_list())\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_locs = defaultdict(list)\n",
    "agg_values = defaultdict(list)\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "\n",
    "zero_shot_vals = zero_shot_transfer_baseline[metric].to_numpy()\n",
    "zero_shot_error = zero_shot_vals - zero_shot_transfer_baseline[f\"{lower_metric}_lb\"].to_numpy()\n",
    "\n",
    "ax.errorbar(\n",
    "    x=np.arange(cur_x, cur_x + zero_shot_vals.shape[0]),\n",
    "    y=zero_shot_vals,\n",
    "    yerr=zero_shot_error,\n",
    "    fmt='o',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=0.20,\n",
    "    zorder=1,\n",
    "    markersize=markersize_minor,\n",
    "    )\n",
    "\n",
    "ax.errorbar(\n",
    "    np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean(),\n",
    "    aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}\"],\n",
    "    yerr=aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}_ub\"]-aggregated_df.xs((0.0, \"subgraphs\"))[f\"{lower_metric}\"],\n",
    "    fmt='D',\n",
    "    color=checkpoint_cmap[\"zero_shot_transfer\"],\n",
    "    alpha=1.0,\n",
    "    label=learning_algorithm,\n",
    "    zorder=2,\n",
    "    markersize=markersize_major,\n",
    "    fillstyle=\"none\",\n",
    "    )\n",
    "\n",
    "ax.hlines(\n",
    "    y=zero_shot_vals.mean(), \n",
    "    xmin=0,\n",
    "    xmax=1000,\n",
    "    colors=[\"gray\"],\n",
    "    zorder=1,\n",
    "    linestyles=\"--\",\n",
    "    alpha=0.75,\n",
    "    )\n",
    "\n",
    "x_ticks += [np.arange(cur_x, cur_x + zero_shot_vals.shape[0]).mean()]\n",
    "x_tick_labels += [\"0\"]\n",
    "\n",
    "cur_x = zero_shot_vals.shape[0] + 35\n",
    "\n",
    "for k in all_k_shots:\n",
    "    matching_k = high_adaptation_checkpoints[\"k\"] == k\n",
    "    \n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        \n",
    "        if learning_algorithm not in learning_algorithms_with_values:\n",
    "            continue\n",
    "        \n",
    "        matching_learning_alg = matching_k & (high_adaptation_checkpoints[\"learning_algorithm\"] == learning_algorithm)\n",
    "        \n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (high_adaptation_checkpoints[\"checkpoint\"] == ckpt)\n",
    "            \n",
    "            if high_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif high_adaptation_checkpoints[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "                        \n",
    "            value = high_adaptation_checkpoints[matching_ckpt][lower_metric].item()\n",
    "            value_error = value - high_adaptation_checkpoints[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            \n",
    "            ax.errorbar(cur_x, value, yerr=value_error, fmt='o', color=cmap(i), alpha=0.20, zorder=1, markersize=markersize_minor)\n",
    "            \n",
    "            cur_x += 1\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "        agg_value = aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "        agg_error = aggregated_df.xs((k, learning_algorithm))[f\"{metric}_ub\"] - aggregated_df.xs((k, learning_algorithm))[metric]\n",
    "\n",
    "        agg_locs[learning_algorithm] += [agg_loc]\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "        \n",
    "        ax.errorbar(agg_loc, agg_value, yerr=agg_error, fmt='D', color=cmap(i), alpha=1.0, label=learning_algorithm, zorder=2, markersize=markersize_major, fillstyle=\"none\",)\n",
    "        \n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "    \n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xlim(-25, cur_x)\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=cmap(0), lw=4),\n",
    "    Line2D([0], [0], color=cmap(1), lw=4),\n",
    "    Line2D([0], [0], color=cmap(2), lw=4),\n",
    "    Line2D([0], [0], color=cmap(3), lw=4),\n",
    "    Line2D([0], [0], color=cmap(4), lw=4),\n",
    "    ]\n",
    "\n",
    "#ax.legend(custom_lines, list(checkpoints.keys()))\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks[1:]), max(x_ticks[1:]), step=1, dtype=float)\n",
    "all_k_shots_x = np.stack([np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1)\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks[1:], dtype=float)\n",
    "    \n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"HealthStory\\n(low adaptation)\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\\nHigh Adaptation\", fontsize=fontsize_major)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset GATs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    # These were not trained as MAML_RH\n",
    "    # As these are reset models\n",
    "    # But the fairest comparsion is to MAML_RH\n",
    "    \"maml_rh\": [\"vb49pmtr\", \"8kixi0s8\", \"vzswebea\", \"a4xe91oq\", \"6syunm30\"],\n",
    "    \"protomaml\": [\"9o4wp36l\", \"ouxd7twt\", \"aimkj4sa\", \"euh2mnqo\", \"p33ybhsn\"],\n",
    "}\n",
    "\n",
    "reset_checkpoint_results = []\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    print(learning_algorithm)\n",
    "    for checkpoint_seed, checkpoint in enumerate(ckpts):\n",
    "\n",
    "        filters = {\n",
    "            \"dataset\": \"CoAID\",\n",
    "            \"top_users_excluded\": 0,\n",
    "            \"version\": f\"reset_{checkpoint}_{checkpoint_seed}\",\n",
    "            \"structure_mode\": \"transductive\",\n",
    "        }\n",
    "\n",
    "        raw_results, checkpoint_results, checkpoint_results_weighted = get_weighted_results_table(\n",
    "            filters=filters,\n",
    "            metrics=metrics,\n",
    "            split=\"test\",\n",
    "            alpha=0.10\n",
    "        )\n",
    "\n",
    "        checkpoint_results_weighted[\"checkpoint\"] = checkpoint\n",
    "        checkpoint_results_weighted[\"learning_algorithm\"] = learning_algorithm\n",
    "\n",
    "        reset_checkpoint_results.append(checkpoint_results_weighted)\n",
    "\n",
    "#reset_checkpoint_results = reset_checkpoint_results[0]\n",
    "reset_checkpoint_results = pd.concat(reset_checkpoint_results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_checkpoint_results = reset_checkpoint_results[reset_checkpoint_results[\"class_weights\"] == (1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "aggregated_df = normalize_by_group(\n",
    "    df=reset_checkpoint_results,\n",
    "    by=[\"k\", \"learning_algorithm\"],\n",
    "    metric=metric,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "x_ticks = []\n",
    "x_tick_labels = []\n",
    "\n",
    "agg_values = defaultdict(list)\n",
    "custom_lines = []\n",
    "\n",
    "prev_k_loc = 0\n",
    "cur_x = 0\n",
    "for k in all_k_shots:\n",
    "    matching_k = reset_checkpoint_results[\"k\"] == k\n",
    "\n",
    "    for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "        matching_learning_alg = matching_k & (\n",
    "            reset_checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "        )\n",
    "\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        for ckpt in ckpts:\n",
    "            matching_ckpt = matching_learning_alg & (\n",
    "                reset_checkpoint_results[\"checkpoint\"] == ckpt\n",
    "            )\n",
    "\n",
    "            if reset_checkpoint_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                raise KeyboardInterrupt()\n",
    "            elif reset_checkpoint_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                cur_x += 1\n",
    "                continue\n",
    "\n",
    "            value = reset_checkpoint_results[matching_ckpt][lower_metric].item()\n",
    "            value_error = (\n",
    "                value\n",
    "                - reset_checkpoint_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "            )\n",
    "\n",
    "            ax.errorbar(\n",
    "                cur_x,\n",
    "                value,\n",
    "                yerr=value_error,\n",
    "                fmt=\"o\",\n",
    "                color=color,\n",
    "                alpha=0.20,\n",
    "                zorder=0,\n",
    "                markersize=markersize_minor,\n",
    "            )\n",
    "\n",
    "            cur_x += 1\n",
    "\n",
    "        agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "        agg_value = agg_row[metric].item()\n",
    "        agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "        agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "        ax.errorbar(\n",
    "            agg_loc,\n",
    "            agg_value,\n",
    "            yerr=agg_error,\n",
    "            fmt=\"D\",\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            label=learning_algorithm,\n",
    "            zorder=2,\n",
    "            markersize=markersize_major,\n",
    "            fillstyle=\"none\",\n",
    "        )\n",
    "\n",
    "        agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "        custom_lines += [\n",
    "            Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "        ]\n",
    "\n",
    "        cur_x += 5\n",
    "\n",
    "    x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "    x_tick_labels += [k]\n",
    "\n",
    "    cur_x += 25\n",
    "    prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=len(all_k_shots) * len(checkpoints), dtype=float)\n",
    "all_k_shots_x = np.stack(\n",
    "    [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    ")\n",
    "\n",
    "for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "    all_k_shots = np.array(x_ticks, dtype=float)\n",
    "\n",
    "    ax.plot(all_k_shots, values, c=color, alpha=0.75, zorder=1)\n",
    "\n",
    "#ax.set_title(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "ax.legend(\n",
    "    custom_lines,\n",
    "    list(checkpoints.keys()),\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.15),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=3,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_dfs = []\n",
    "for metric in [\"f1_0\", \"aupr_0\", \"f1_1\", \"aupr_1\", \"mcc\"]:\n",
    "    all_agg_dfs += [normalize_by_group(\n",
    "        df=reset_checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "        remove={\"var\", \"se\", \"N\"}\n",
    "    )]\n",
    "    \n",
    "all_agg_dfs = pd.concat(\n",
    "    all_agg_dfs,\n",
    "    axis=1,\n",
    "    join=\"outer\"\n",
    "    ).reset_index()\n",
    "\n",
    "all_agg_dfs[\"learning_algorithm\"] = pd.Series(pd.Categorical(\n",
    "    values=all_agg_dfs[\"learning_algorithm\"],\n",
    "    categories=list(checkpoints.keys()),\n",
    "    ordered=True\n",
    "    ))\n",
    "\n",
    "all_agg_dfs = all_agg_dfs.sort_values(by=[\"k\", \"learning_algorithm\"]).set_index(keys=[\"k\", \"learning_algorithm\"])\n",
    "\n",
    "all_agg_dfs.to_clipboard(excel=True,)\n",
    "\n",
    "all_agg_dfs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_reset\", reset_checkpoint_results), (\"gat\", high_adaptation_checkpoints)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"<\" if checkpoint_type == \"gat_reset\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='<', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=11)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"MAML\", \"ProtoMAML\"] + [\"Reset\", \"GAT\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=4,\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/coaid_random_transfer_comparison_mcc.pdf\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"aupr_1\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]))\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat_reset\", reset_checkpoint_results), (\"gat\", high_adaptation_checkpoints)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                checkpoint_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.errorbar(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                yerr=agg_error,\n",
    "                fmt=\"<\" if checkpoint_type == \"gat_reset\" else \"v\",\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                markersize=markersize_major,\n",
    "                fillstyle=\"none\",\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        all_k_shots_ = np.array(x_ticks, dtype=float)\n",
    "\n",
    "        ax.plot(\n",
    "            all_k_shots_,\n",
    "            values,\n",
    "            c=color,\n",
    "            alpha=0.75,\n",
    "            zorder=1,\n",
    "            ls=\"-\" if checkpoint_type == \"gat\" else \"--\"\n",
    "            )\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "for learning_algorithm, ckpts in checkpoints.items():\n",
    "    \n",
    "    color = checkpoint_cmap[learning_algorithm]\n",
    "    \n",
    "    custom_lines += [\n",
    "        Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "    ]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='<', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "    Line2D([0], [0], marker='^', markerfacecolor=\"k\", color='w', lw=0, markersize=markersize_major+2),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=11)\n",
    "ax.set_ylabel(\"AUPR-Fake\", fontsize=fontsize_major)\n",
    "ax.set_xlabel(\"$k$-shot\", fontsize=fontsize_major)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "#ax.legend(\n",
    "#    custom_lines,\n",
    "#    list(checkpoints.keys()) + [\"Reset\", \"GAT\"],\n",
    "#    loc='upper center',\n",
    "#    bbox_to_anchor=(0.5, 1.25),\n",
    "#    fontsize=fontsize_minor,\n",
    "#    ncol=4,\n",
    "#    )\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/coaid_random_transfer_comparison_aupr_1.pdf\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoAID & Twitter Transfer Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.style as style\n",
    "\n",
    "#style.use('tableau-colorblind10')\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "checkpoint_cmap = {\n",
    "    \"zero_shot_transfer\": cmap(0),\n",
    "    \"subgraphs\": cmap(0),\n",
    "    \"maml_lh\": cmap(1),\n",
    "    \"maml_rh\": cmap(2),\n",
    "    \"prototypical\": cmap(3),\n",
    "    \"protomaml\": cmap(4),\n",
    "}\n",
    "\n",
    "checkpoint_fmt = {\n",
    "    \"zero_shot_transfer\": \"o\",\n",
    "    \"subgraphs\": \"o\",\n",
    "    \"maml_lh\": \"v\",\n",
    "    \"maml_rh\": \"^\",\n",
    "    \"prototypical\": \"d\",\n",
    "    \"protomaml\": \"D\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "markersize_minor = 2\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(2 * figsize[0], figsize[1]), layout=\"constrained\")\n",
    "\n",
    "custom_lines = []\n",
    "for ax_num, (dataset_name, transfer_results, ax) in enumerate(zip([\"CoAID\", \"TwitterHateSpeech\"], [coaid_transfer, twitter_transfer], axes)):\n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=transfer_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    if dataset_name in {\"CoAID\"}:\n",
    "        zero_shot_vals = zero_shot_transfer_baseline[metric].to_numpy()\n",
    "\n",
    "        ax.hlines(\n",
    "            y=zero_shot_vals.mean(), \n",
    "            xmin=0,\n",
    "            xmax=1000,\n",
    "            colors=[\"gray\"],\n",
    "            zorder=1,\n",
    "            linestyles=\"--\",\n",
    "            alpha=0.75,\n",
    "            )\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = transfer_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            matching_learning_alg = matching_k & (\n",
    "                transfer_results[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "            fmt = checkpoint_fmt[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                matching_ckpt = matching_learning_alg & (\n",
    "                    transfer_results[\"checkpoint\"] == ckpt\n",
    "                )\n",
    "\n",
    "                if transfer_results[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                    raise KeyboardInterrupt()\n",
    "                elif transfer_results[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                    cur_x += 1\n",
    "                    continue\n",
    "\n",
    "                value = transfer_results[matching_ckpt][lower_metric].item()\n",
    "                value_error = (\n",
    "                    value\n",
    "                    - transfer_results[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "                )\n",
    "\n",
    "                ax.errorbar(\n",
    "                    cur_x,\n",
    "                    value,\n",
    "                    yerr=value_error,\n",
    "                    fmt=fmt,\n",
    "                    color=color,\n",
    "                    alpha=0.25,\n",
    "                    zorder=1,\n",
    "                    markersize=markersize_minor,\n",
    "                )\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.scatter(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                marker=fmt,\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                s=5 * markersize_major,\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            if ax_num == 0:\n",
    "                custom_lines += [\n",
    "                    Line2D([0], [0], marker=fmt, markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "                ]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 10\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "    all_k_shots_range = np.arange(min(x_ticks), max(x_ticks), step=1, dtype=float)\n",
    "    all_k_shots_x = np.stack(\n",
    "        [np.ones_like(all_k_shots_range, dtype=float), all_k_shots_range], axis=1\n",
    "    )\n",
    "\n",
    "    for i, (learning_algorithm, values) in enumerate(agg_values.items()):\n",
    "        color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "        #ax.plot(x_ticks, values, c=color, alpha=1, zorder=0, linewidth=1)\n",
    "\n",
    "    ax.set_title(\" \", fontsize=16)\n",
    "    \n",
    "    ax.set_xlim(-10, cur_x)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_tick_labels, fontsize=fontsize_minor)\n",
    "\n",
    "    ax.set_ylim(0, 0.25)\n",
    "    ax.set_yticks([0.0, 0.05, 0.10, 0.15, 0.20, 0.25])\n",
    "    ax.set_yticklabels([\"0.00\", \"0.05\", \"0.10\", \"0.15\", \"0.20\", \"0.25\"], fontsize=fontsize_minor)\n",
    "    \n",
    "    ax.set_xlabel(dataset_name, fontsize=fontsize_major)\n",
    "    if ax_num == 0:\n",
    "        ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    #ax.tick_params(axis='both', which='major', labelsize=fontsize_minor)\n",
    "\n",
    "fig.legend(\n",
    "    custom_lines,\n",
    "    [\"Subgraphs\", \"MAML-LH\", \"MAML-RH\", \"ProtoNet\", \"ProtoMAML\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=5,\n",
    "    )\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/coaid_and_twitter_transfer_line.pdf\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme $k$-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mcc\"\n",
    "\n",
    "lower_metric = metric.lower()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2 * figsize[0], figsize[1]), layout=\"constrained\")\n",
    "\n",
    "tested_learning_algorithms = [\n",
    "    \"prototypical\"\n",
    "]\n",
    "\n",
    "all_k_shots = [4, 8, 12, 16, 32, 64, 128, 256]\n",
    "\n",
    "custom_lines = []\n",
    "\n",
    "for checkpoint_type, checkpoint_results in [(\"gat\", twitter_transfer)]:\n",
    "    \n",
    "    aggregated_df = normalize_by_group(\n",
    "        df=checkpoint_results,\n",
    "        by=[\"k\", \"learning_algorithm\"],\n",
    "        metric=metric,\n",
    "    )\n",
    "    \n",
    "    aggregated_df = aggregated_df.loc[pd.IndexSlice[:, tested_learning_algorithms], :]\n",
    "    \n",
    "    prev_k_loc = 0\n",
    "    cur_x = 0\n",
    "\n",
    "    x_ticks = []\n",
    "    x_tick_labels = []\n",
    "\n",
    "    agg_values = defaultdict(list)\n",
    "\n",
    "    for k in all_k_shots:\n",
    "        matching_k = checkpoint_results[\"k\"] == k\n",
    "\n",
    "        for i, (learning_algorithm, ckpts) in enumerate(checkpoints.items()):\n",
    "            if learning_algorithm not in {\"prototypical\"}:\n",
    "                continue\n",
    "\n",
    "            matching_learning_alg = matching_k & (\n",
    "                twitter_transfer[\"learning_algorithm\"] == learning_algorithm\n",
    "            )\n",
    "\n",
    "            color = checkpoint_cmap[learning_algorithm]\n",
    "            fmt = checkpoint_fmt[learning_algorithm]\n",
    "\n",
    "            for ckpt in ckpts:\n",
    "                matching_ckpt = matching_learning_alg & (\n",
    "                    twitter_transfer[\"checkpoint\"] == ckpt\n",
    "                )\n",
    "\n",
    "                if twitter_transfer[matching_ckpt][lower_metric].shape[0] > 1:\n",
    "                    raise KeyboardInterrupt()\n",
    "                elif twitter_transfer[matching_ckpt][lower_metric].shape[0] == 0:\n",
    "                    cur_x += 1\n",
    "                    continue\n",
    "\n",
    "                value = twitter_transfer[matching_ckpt][lower_metric].item()\n",
    "                value_error = (\n",
    "                    value\n",
    "                    - twitter_transfer[matching_ckpt][f\"{lower_metric}_lb\"].item()\n",
    "                )\n",
    "\n",
    "                ax.errorbar(\n",
    "                    cur_x,\n",
    "                    value,\n",
    "                    yerr=value_error,\n",
    "                    fmt=fmt,\n",
    "                    color=color,\n",
    "                    alpha=0.25,\n",
    "                    zorder=1,\n",
    "                    markersize=markersize_minor,\n",
    "                )\n",
    "                cur_x += 1\n",
    "\n",
    "            agg_row = aggregated_df.xs(key=(k, learning_algorithm))\n",
    "            agg_value = agg_row[metric].item()\n",
    "            agg_error = (agg_row[f\"{metric}_ub\"] - agg_row[metric]).item()\n",
    "\n",
    "            agg_loc = cur_x - len(ckpts) / 2\n",
    "\n",
    "            ax.scatter(\n",
    "                agg_loc,\n",
    "                agg_value,\n",
    "                marker=fmt,\n",
    "                color=color,\n",
    "                alpha=1.0,\n",
    "                label=learning_algorithm,\n",
    "                zorder=2,\n",
    "                s=5 * markersize_major,\n",
    "            )\n",
    "\n",
    "            agg_values[learning_algorithm] += [agg_value]\n",
    "\n",
    "            cur_x += 5\n",
    "\n",
    "        x_ticks += [(prev_k_loc + cur_x) / 2]\n",
    "        x_tick_labels += [k]\n",
    "\n",
    "        cur_x += 25\n",
    "        prev_k_loc = cur_x\n",
    "\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "learning_algorithm = \"prototypical\"\n",
    "    \n",
    "color = checkpoint_cmap[learning_algorithm]\n",
    "\n",
    "custom_lines += [\n",
    "    Line2D([0], [0], marker='D', markerfacecolor=color, color='w', lw=0, markersize=markersize_major),\n",
    "]\n",
    "\n",
    "ax.set_title(\" \", fontsize=16)\n",
    "\n",
    "ax.set_xlim(-10, cur_x-20)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_tick_labels, fontsize=fontsize_minor)\n",
    "\n",
    "ax.set_ylim(0, 0.25)\n",
    "ax.set_yticks([0.0, 0.05, 0.10, 0.15, 0.20, 0.25])\n",
    "ax.set_yticklabels([\"0.00\", \"0.05\", \"0.10\", \"0.15\", \"0.20\", \"0.25\"], fontsize=fontsize_minor)\n",
    "\n",
    "ax.set_xlabel(dataset_name, fontsize=fontsize_major)\n",
    "ax.set_ylabel(\"MCC\", fontsize=fontsize_major)\n",
    "\n",
    "fig.legend(\n",
    "    [\n",
    "        Line2D([0], [0], marker=fmt, markerfacecolor=color, color='w', lw=0, markersize=markersize_major)\n",
    "        ],\n",
    "    [\"ProtoNet\"],\n",
    "    loc='upper center',\n",
    "    #bbox_to_anchor=(0.5, 1.25),\n",
    "    fontsize=fontsize_minor,\n",
    "    ncol=5,\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/twitter_extreme_kshot_mcc.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-learning-gnns-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
