{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {\"bbox_inches\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_destination = \"paper\"\n",
    "\n",
    "if figure_destination == \"paper\":\n",
    "    figsize = (6.30045, 0.9*9.72632)\n",
    "    fontsize_major = 9\n",
    "    fontsize_minor = 7\n",
    "    markersize_minor = 2\n",
    "    markersize_major = 6\n",
    "\n",
    "elif figure_destination == \"slide\":\n",
    "    figsize = (6.10, 4.87)\n",
    "    fontsize_major = 16\n",
    "    fontsize_minor = 11\n",
    "    markersize_minor = 4\n",
    "    markersize_major = 8    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import assortativity, k_hop_subgraph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def compute_graph_stats(graph):\n",
    "    num_classes = len([k for k in graph.keys() if \"locs\" in k])\n",
    "\n",
    "    label_locs = [graph[f\"label_{l}_locs\"].flatten() for l in range(num_classes)]\n",
    "    all_label_locs = torch.cat(label_locs).flatten()\n",
    "\n",
    "    label_counts = torch.tensor([len(label_locs[l]) for l in range(num_classes)])\n",
    "    label_prop = label_counts / label_counts.sum()\n",
    "\n",
    "    local_adj_matrix = torch.zeros(\n",
    "        (num_classes, num_classes),\n",
    "        )\n",
    "\n",
    "    excess_homophily = [[] for l in range(num_classes)]\n",
    "\n",
    "    # Iterate over all nodes with (potential) labels\n",
    "    # Then iterate over all other potential labels nodes\n",
    "    # Check if the other label nodes are in the 2-hop neighbourhood\n",
    "    # i.e. check if other label nodes can influence the representations of label nodes\n",
    "    for l, lbl_locs in enumerate(label_locs):\n",
    "        # For each potential label, get the 2-hop subgraph\n",
    "        for label_loc in lbl_locs.tolist():\n",
    "            subset, _, _, _ = k_hop_subgraph(\n",
    "                node_idx=label_loc, \n",
    "                num_hops=2,\n",
    "                edge_index=graph[\"edge_index\"],\n",
    "            )\n",
    "\n",
    "            subset_local_adj_matrix = torch.zeros((num_classes, ))\n",
    "\n",
    "            # Find all other label nodes in that subgraph\n",
    "            for ll, lbl_locs2 in enumerate(label_locs):\n",
    "                # The -1 is to remove self-edges\n",
    "                ll_neighbourhood_size = torch.isin(\n",
    "                    subset,\n",
    "                    lbl_locs2,\n",
    "                    assume_unique=True\n",
    "                ).sum() #- (1 if l == ll else 0)\n",
    "                \n",
    "                local_adj_matrix[l, ll] += ll_neighbourhood_size\n",
    "                \n",
    "                subset_local_adj_matrix[ll] = ll_neighbourhood_size\n",
    "\n",
    "            # Find the number of homophilic labels and subtract the label propensity from it\n",
    "            # Thus, the excess homophily for the current labelled node\n",
    "            subset_excess_homophily = subset_local_adj_matrix[l] / subset_local_adj_matrix.sum() - label_prop[l]\n",
    "            \n",
    "            # Normalize relative to a perfectly homophilic label node\n",
    "            subset_excess_homophily = subset_excess_homophily / (1 - label_prop[l])\n",
    "\n",
    "            if not torch.isnan(subset_excess_homophily):\n",
    "                excess_homophily[l] += [subset_excess_homophily]\n",
    "\n",
    "    # Compute the mean relative excess homophily for each class separately\n",
    "    mean_rel_excess_homophily = list(map(lambda x: torch.mean(torch.stack(x)).item(), excess_homophily))\n",
    "    median_rel_excess_homophily = list(map(lambda x: torch.median(torch.stack(x)).item(), excess_homophily))\n",
    "    min_rel_excess_homophily = list(map(lambda x: torch.min(torch.stack(x)).item(), excess_homophily))\n",
    "    max_rel_excess_homophily = list(map(lambda x: torch.max(torch.stack(x)).item(), excess_homophily))\n",
    "\n",
    "    # Compute graph homophilly\n",
    "    h = torch.diag(local_adj_matrix) / local_adj_matrix.sum(dim=1)\n",
    "    graph_homophily = (torch.sum(torch.clip(h - label_prop, min=0)) / (num_classes - 1)).item()\n",
    "\n",
    "    # Compute graph assortativity\n",
    "    mixing_matrix = local_adj_matrix / local_adj_matrix.sum()\n",
    "\n",
    "    dot_prod_marginals = torch.dot(torch.sum(mixing_matrix, dim=0), torch.sum(mixing_matrix, dim=1))\n",
    "    assortativity = (torch.trace(mixing_matrix) - dot_prod_marginals) / (1 - dot_prod_marginals)\n",
    "    assortativity = assortativity.item()\n",
    "\n",
    "    # Use networkx for some other standard graph statistics\n",
    "    all_label_locs = all_label_locs.tolist()\n",
    "    \n",
    "    num_nodes = graph[\"num_nodes\"].item()\n",
    "    num_edges = graph[\"num_edges\"].item()\n",
    "    num_labels = len(all_label_locs)\n",
    "    density = (2 * num_edges) / (num_nodes * (num_nodes - 1))\n",
    "    \n",
    "    networkx_graph = torch_geometric.utils.to_networkx(\n",
    "        torch_geometric.data.Data(edge_index=graph[\"edge_index\"], num_nodes=graph[\"num_nodes\"]),\n",
    "        to_undirected=True\n",
    "        )\n",
    "\n",
    "    degree_centrality = nx.degree_centrality(networkx_graph)\n",
    "    degree_centrality = np.mean([degree_centrality[i] for i in all_label_locs])\n",
    "\n",
    "    eigen_centrality = nx.eigenvector_centrality_numpy(networkx_graph, max_iter=10)\n",
    "    eigen_centrality = np.mean([eigen_centrality[i] for i in all_label_locs])\n",
    "\n",
    "    graph_stats = {\n",
    "            \"homophily\": graph_homophily,\n",
    "            \"assortativity\": assortativity,\n",
    "            \"mean_rel_excess_homophily\": mean_rel_excess_homophily,\n",
    "            \"num_nodes\": num_nodes,\n",
    "            \"num_edges\": num_edges,\n",
    "            \"num_labels\": num_labels,\n",
    "            \"graph_density\": density,\n",
    "            \"label_density\": num_labels / num_nodes,\n",
    "            \"degree_centrality\": degree_centrality,\n",
    "            \"eigen_centrality\": eigen_centrality,\n",
    "        }\n",
    "\n",
    "    return graph_stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gossipcop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"./data/structured/gossipcop/seed[942]_splits[5]_minlen[0]_filterisolated[True]_topk[30]_topexcl[1]_userdoc[30]_featuretype[one-hot]_vocab[joint][random][10000x768]_userfeatures[post][zeros]/0\"\n",
    "\n",
    "subdirs = next(os.walk(loc))[1]\n",
    "for subdir in subdirs:\n",
    "    \n",
    "    split = re.search(\"split\\=(.+?), \", subdir).group(1)\n",
    "    \n",
    "    version  = re.search(\"version\\=(.+?)\\)\", subdir)\n",
    "    \n",
    "    if version is None:\n",
    "        continue\n",
    "    else:\n",
    "        meta_split = version.group(1)\n",
    "    \n",
    "    if split == \"train\" and meta_split == \"meta_train_support\":\n",
    "        support_batches_loc = loc + f\"/{subdir}\"\n",
    "    elif split == \"train\" and meta_split == \"meta_train_query\":\n",
    "        query_batches_loc = loc + f\"/{subdir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_graph_stats = []\n",
    "\n",
    "batch_locs = next(os.walk(support_batches_loc))[2]\n",
    "\n",
    "for i, batched_graph in enumerate(tqdm(batch_locs)):\n",
    "    \n",
    "    batched_graph_loc = support_batches_loc + f\"/{batched_graph}\"\n",
    "    \n",
    "    graph = torch.load(batched_graph_loc, map_location=\"cpu\")\n",
    "\n",
    "    support_graph_stats += [compute_graph_stats(graph)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "records = list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 0,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][0],\n",
    "            },\n",
    "        support_graph_stats\n",
    "        )\n",
    "    )\n",
    "\n",
    "records += list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 1,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][1],\n",
    "            },\n",
    "        support_graph_stats\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=ax,\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"\")\n",
    "#ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "\n",
    "ax.set_ylabel(\"Gossipcop\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_title(\"Support\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/gossipcop_support.pdf\")\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/gossipcop_support.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(\n",
    "    x=list(map(lambda x: x[\"homophily\"], support_graph_stats)),\n",
    "    bw_adjust=0.8,\n",
    "    clip=(0, 1),\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    )\n",
    "\n",
    "ax.set_xlim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(\n",
    "    x=list(map(lambda x: x[\"assortativity\"], support_graph_stats)),\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-1, 1),\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/gossipcop_support.pickle\", \"wb\") as f:\n",
    "    pickle.dump(support_graph_stats, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/structured/gossipcop/seed[942]_splits[5]_minlen[0]_filterisolated[True]_topk[30]_topexcl[1]_userdoc[30]_featuretype[one-hot]_vocab[joint][random][10000x768]_userfeatures[post][zeros]/0/socialgraph(mode=inductive, split=train, keep_cc=largest).pickle\", \"rb\") as f:\n",
    "    graph_dataset = pickle.load(f)\n",
    "\n",
    "num_classes = graph_dataset[\"num_classes\"]\n",
    "\n",
    "graph_labels = graph_dataset[\"graph\"][\"y\"]\n",
    "label_locs = [torch.where(graph_labels == l)[0] for l in range(num_classes)]\n",
    "all_label_locs = torch.cat(label_locs)\n",
    "\n",
    "label_prop = torch.tensor(list(map(lambda x: x.shape[0], label_locs)))\n",
    "label_prop = label_prop / label_prop.sum()\n",
    "\n",
    "num_nodes = graph_dataset[\"graph\"].num_nodes\n",
    "\n",
    "all_num_nodes = []\n",
    "all_num_edges = []\n",
    "all_density = []\n",
    "all_num_labels = []\n",
    "\n",
    "rel_excess_homophily = {l: [] for l in range(num_classes)}\n",
    "\n",
    "max_iterations = min(label_locs[l].shape[0] for l in range(num_classes))\n",
    "\n",
    "for l in range(num_classes):\n",
    "    for i, label_loc in enumerate(label_locs[l]):\n",
    "        \n",
    "        subset, edge_index, _, _ = k_hop_subgraph(\n",
    "            node_idx=label_loc.unsqueeze(0),\n",
    "            edge_index=graph_dataset[\"graph\"][\"edge_index\"],\n",
    "            num_hops=2,\n",
    "        )\n",
    "\n",
    "        subset_local_adj_matrix = torch.zeros((num_classes, ))\n",
    "\n",
    "        for ll in range(num_classes):\n",
    "\n",
    "            ll_label_nodes = torch.isin(\n",
    "                test_elements=subset,\n",
    "                elements=label_locs[ll],\n",
    "                assume_unique=True,\n",
    "            ).sum()\n",
    "\n",
    "            subset_local_adj_matrix[ll] += ll_label_nodes\n",
    "\n",
    "        all_num_nodes += [subset.shape[0]]\n",
    "        all_num_edges += [edge_index.shape[1]]\n",
    "        all_density += [(2 * all_num_edges[-1]) / (all_num_nodes[-1] * (all_num_nodes[-1] - 1))]\n",
    "        all_num_labels += [subset_local_adj_matrix.sum()]\n",
    "\n",
    "        subset_local_adj_matrix = subset_local_adj_matrix / subset_local_adj_matrix.sum()\n",
    "        local_rel_excess_homophily = (subset_local_adj_matrix - label_prop) / (1 - label_prop)\n",
    "\n",
    "        rel_excess_homophily[l] += [local_rel_excess_homophily[l].item()]\n",
    "\n",
    "        if i == max_iterations-1:\n",
    "            break\n",
    "\n",
    "networkx_graph = torch_geometric.utils.to_networkx(\n",
    "    graph_dataset[\"graph\"],\n",
    "    to_undirected=True\n",
    "    )\n",
    "\n",
    "degree_centrality = nx.degree_centrality(networkx_graph)\n",
    "degree_centrality = [degree_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "eigen_centrality = nx.eigenvector_centrality_numpy(networkx_graph, max_iter=10)\n",
    "eigen_centrality = [eigen_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "del graph_dataset, networkx_graph\n",
    "\n",
    "graph_stats = {\n",
    "    \"mean_rel_excess_homophily\": rel_excess_homophily,\n",
    "    \"num_nodes\": all_num_nodes,\n",
    "    \"num_edges\": all_num_edges,\n",
    "    \"num_labels\": all_num_labels,\n",
    "    \"graph_density\": all_density,\n",
    "    \"label_density\": [(num_labels / num_nodes).item() for num_labels, num_nodes in zip(all_num_labels, all_num_nodes)],\n",
    "    \"degree_centrality\": degree_centrality,\n",
    "    \"eigen_centrality\": eigen_centrality,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"label\": l, \"excess_homophily\": metric_val}\n",
    "    for l, metric_vals in rel_excess_homophily.items()\n",
    "    for metric_val in metric_vals\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=ax,\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"\")\n",
    "#ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "ax.get_legend().remove()\n",
    "\n",
    "ax.set_ylabel(\"\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_title(\"Query\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/gossipcop_query.pdf\")\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/gossipcop_query.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/gossipcop_query.pickle\", \"wb\") as f:\n",
    "    pickle.dump(graph_stats, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Hate Speech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"./data/structured/twitterHateSpeech/seed[942]_splits[5]_minlen[0]_filterisolated[True]_topk[30]_topexcl[0]_userdoc[100]_featuretype[one-hot]_vocab[joint][random][10000x768]_userfeatures[post][zeros]/0\"\n",
    "\n",
    "subdirs = next(os.walk(loc))[1]\n",
    "for subdir in subdirs:\n",
    "    \n",
    "    split = re.search(\"split\\=(.+?), \", subdir).group(1)\n",
    "    \n",
    "    version  = re.search(\"version\\=(.+?)\\)\", subdir)\n",
    "    \n",
    "    if version is None:\n",
    "        continue\n",
    "    else:\n",
    "        meta_split = version.group(1)\n",
    "    \n",
    "    if split == \"train\" and meta_split == \"meta_train_support\":\n",
    "        support_batches_loc = loc + f\"/{subdir}\"\n",
    "    elif split == \"train\" and meta_split == \"meta_train_query\":\n",
    "        query_batches_loc = loc + f\"/{subdir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_graph_stats = []\n",
    "\n",
    "batch_locs = next(os.walk(support_batches_loc))[2]\n",
    "for i, batched_graph in enumerate(tqdm(batch_locs)):\n",
    "    \n",
    "    batched_graph_loc = support_batches_loc + f\"/{batched_graph}\"\n",
    "    \n",
    "    graph = torch.load(batched_graph_loc, map_location=\"cpu\")\n",
    "\n",
    "    support_graph_stats += [compute_graph_stats(graph)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "records = list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 0,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][0],\n",
    "            },\n",
    "        support_graph_stats\n",
    "        )\n",
    "    )\n",
    "\n",
    "records += list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 1,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][1],\n",
    "            },\n",
    "        support_graph_stats\n",
    "        )\n",
    "    )\n",
    "\n",
    "records += list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 2,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][2],\n",
    "            },\n",
    "        support_graph_stats\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=ax,\n",
    "    palette=\"tab10\"\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "#ax.set_xticklabels([])\n",
    "#ax.set_xlabel(\"\")\n",
    "ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "\n",
    "ax.set_ylabel(\"Twitter Hate Speech\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "#ax.set_title(\"Support\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/twitterHateSpeech_support.pdf\")\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/twitterHateSpeech_support.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/twitterhatespeech_support.pickle\", \"wb\") as f:\n",
    "    pickle.dump(support_graph_stats, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/structured/twitterHateSpeech/seed[942]_splits[5]_minlen[0]_filterisolated[True]_topk[30]_topexcl[0]_userdoc[100]_featuretype[one-hot]_vocab[joint][random][10000x768]_userfeatures[post][zeros]/0/socialgraph(mode=inductive, split=train, keep_cc=largest).pickle\", \"rb\") as f:\n",
    "    graph_dataset = pickle.load(f)\n",
    "\n",
    "num_classes = graph_dataset[\"num_classes\"]\n",
    "\n",
    "graph_labels = graph_dataset[\"graph\"][\"y\"]\n",
    "label_locs = [torch.where(graph_labels == l)[0] for l in range(num_classes)]\n",
    "all_label_locs = torch.cat(label_locs)\n",
    "\n",
    "label_prop = torch.tensor(list(map(lambda x: x.shape[0], label_locs)))\n",
    "label_prop = label_prop / label_prop.sum()\n",
    "\n",
    "num_nodes = graph_dataset[\"graph\"].num_nodes\n",
    "\n",
    "all_num_nodes = []\n",
    "all_num_edges = []\n",
    "all_density = []\n",
    "all_num_labels = []\n",
    "\n",
    "rel_excess_homophily = {l: [] for l in range(num_classes)}\n",
    "\n",
    "max_iterations = min(label_locs[l].shape[0] for l in range(num_classes))\n",
    "\n",
    "for l in range(num_classes):\n",
    "    for i, label_loc in enumerate(label_locs[l]):\n",
    "        \n",
    "        subset, edge_index, _, _ = k_hop_subgraph(\n",
    "            node_idx=label_loc.unsqueeze(0),\n",
    "            edge_index=graph_dataset[\"graph\"][\"edge_index\"],\n",
    "            num_hops=2,\n",
    "        )\n",
    "\n",
    "        subset_local_adj_matrix = torch.zeros((num_classes, ))\n",
    "\n",
    "        for ll in range(num_classes):\n",
    "\n",
    "            ll_label_nodes = torch.isin(\n",
    "                test_elements=subset,\n",
    "                elements=label_locs[ll],\n",
    "                assume_unique=True,\n",
    "            ).sum()\n",
    "\n",
    "            subset_local_adj_matrix[ll] += ll_label_nodes\n",
    "\n",
    "        all_num_nodes += [subset.shape[0]]\n",
    "        all_num_edges += [edge_index.shape[1]]\n",
    "        all_density += [(2 * all_num_edges[-1]) / (all_num_nodes[-1] * (all_num_nodes[-1] - 1))]\n",
    "        all_num_labels += [subset_local_adj_matrix.sum()]\n",
    "\n",
    "        subset_local_adj_matrix = subset_local_adj_matrix / subset_local_adj_matrix.sum()\n",
    "        local_rel_excess_homophily = (subset_local_adj_matrix - label_prop) / (1 - label_prop)\n",
    "\n",
    "        rel_excess_homophily[l] += [local_rel_excess_homophily[l].item()]\n",
    "\n",
    "        #if i == max_iterations-1:\n",
    "        #    break\n",
    "\n",
    "networkx_graph = torch_geometric.utils.to_networkx(\n",
    "    graph_dataset[\"graph\"],\n",
    "    to_undirected=True\n",
    "    )\n",
    "\n",
    "degree_centrality = nx.degree_centrality(networkx_graph)\n",
    "degree_centrality = [degree_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "eigen_centrality = nx.eigenvector_centrality_numpy(networkx_graph, max_iter=10)\n",
    "eigen_centrality = [eigen_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "del graph_dataset, networkx_graph\n",
    "\n",
    "graph_stats = {\n",
    "    \"mean_rel_excess_homophily\": rel_excess_homophily,\n",
    "    \"num_nodes\": all_num_nodes,\n",
    "    \"num_edges\": all_num_edges,\n",
    "    \"num_labels\": all_num_labels,\n",
    "    \"graph_density\": all_density,\n",
    "    \"label_density\": [(num_labels / num_nodes).item() for num_labels, num_nodes in zip(all_num_labels, all_num_nodes)],\n",
    "    \"degree_centrality\": degree_centrality,\n",
    "    \"eigen_centrality\": eigen_centrality,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"label\": l, \"excess_homophily\": metric_val}\n",
    "    for l, metric_vals in rel_excess_homophily.items()\n",
    "    for metric_val in metric_vals\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    stat='density',\n",
    "    #bw_adjust=0.8,\n",
    "    #clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"dodge\",\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette(\"tab10\"),\n",
    "    common_norm=False,\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"\")\n",
    "#ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "ax.get_legend().remove()\n",
    "\n",
    "ax.set_ylabel(\"\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_title(\"Query\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"label\": l, \"excess_homophily\": metric_val}\n",
    "    for l, metric_vals in rel_excess_homophily.items()\n",
    "    for metric_val in metric_vals\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=1,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette(\"tab10\"),\n",
    "    common_norm=False,\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "#ax.set_xticklabels([])\n",
    "#ax.set_xlabel(\"\")\n",
    "ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "ax.get_legend().remove()\n",
    "\n",
    "ax.set_ylabel(\"\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "#ax.set_title(\"Query\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/twitterHateSpeech_query.pdf\")\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/twitterHateSpeech_query.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/twitterhatespeech_query.pickle\", \"wb\") as f:\n",
    "    pickle.dump(graph_stats, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoAID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"./data/structured/CoAID/seed[942]_splits[0]_minlen[0]_filterisolated[True]_topk[30]_topexcl[0]_userdoc[30]_featuretype[lm-embeddings]_vocab[external][roberta-base][NonexNone]_userfeatures[post][zeros]_version[transfer_77c5a6tu]/0\"\n",
    "\n",
    "subdirs = next(os.walk(loc))[1]\n",
    "for subdir in subdirs:\n",
    "    \n",
    "    split = re.search(\"split\\=(.+?), \", subdir).group(1)\n",
    "    \n",
    "    version  = re.search(\"version\\=(.+?)\\)\", subdir)\n",
    "    \n",
    "    if version is None:\n",
    "        continue\n",
    "    else:\n",
    "        meta_split = version.group(1)\n",
    "    \n",
    "    if split == \"test\" and meta_split == \"meta_train_support\":\n",
    "        support_batches_loc = loc + f\"/{subdir}\"\n",
    "    elif split == \"train\" and meta_split == \"meta_train_query\":\n",
    "        query_batches_loc = loc + f\"/{subdir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_graph_stats = []\n",
    "\n",
    "batch_locs = next(os.walk(support_batches_loc))[2]\n",
    "\n",
    "for i, batched_graph in enumerate(tqdm(batch_locs)):\n",
    "    \n",
    "    batched_graph_loc = support_batches_loc + f\"/{batched_graph}\"\n",
    "    \n",
    "    graph = torch.load(batched_graph_loc, map_location=\"cpu\")\n",
    "\n",
    "    support_graph_stats += [compute_graph_stats(graph)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/coaid_support.pickle\", \"wb\") as f:\n",
    "    pickle.dump(support_graph_stats, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/structured/CoAID/seed[942]_splits[0]_minlen[0]_filterisolated[True]_topk[30]_topexcl[0]_userdoc[30]_featuretype[lm-embeddings]_vocab[external][roberta-base][NonexNone]_userfeatures[post][zeros]_version[transfer_77c5a6tu]/0/episodickhopneighbourhoodsocialgraph(mode=transductive, split=test, k_shot=4, prop_query=0.0, max_k_hop=5, budget=2048, doc_k_hop=2).pickle\", \"rb\") as f:\n",
    "    graph_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = graph_dataset[\"num_classes\"]\n",
    "\n",
    "graph_labels = graph_dataset[\"graph\"][\"y\"]\n",
    "label_locs = [torch.where(graph_labels == l)[0] for l in range(num_classes)]\n",
    "all_label_locs = torch.cat(label_locs)\n",
    "\n",
    "label_prop = torch.tensor(list(map(lambda x: x.shape[0], label_locs)))\n",
    "label_prop = label_prop / label_prop.sum()\n",
    "\n",
    "num_nodes = graph_dataset[\"graph\"].num_nodes\n",
    "\n",
    "all_num_nodes = []\n",
    "all_num_edges = []\n",
    "all_density = []\n",
    "all_num_labels = []\n",
    "\n",
    "rel_excess_homophily = {l: [] for l in range(num_classes)}\n",
    "\n",
    "max_iterations = min(label_locs[l].shape[0] for l in range(num_classes))\n",
    "\n",
    "for l in range(num_classes):\n",
    "    for i, label_loc in enumerate(label_locs[l]):\n",
    "        \n",
    "        subset, edge_index, _, _ = k_hop_subgraph(\n",
    "            node_idx=label_loc.unsqueeze(0),\n",
    "            edge_index=graph_dataset[\"graph\"][\"edge_index\"],\n",
    "            num_hops=2,\n",
    "        )\n",
    "\n",
    "        subset_local_adj_matrix = torch.zeros((num_classes, ))\n",
    "\n",
    "        for ll in range(num_classes):\n",
    "\n",
    "            ll_label_nodes = torch.isin(\n",
    "                test_elements=subset,\n",
    "                elements=label_locs[ll],\n",
    "                assume_unique=True,\n",
    "            ).sum()\n",
    "\n",
    "            subset_local_adj_matrix[ll] += ll_label_nodes\n",
    "\n",
    "        all_num_nodes += [subset.shape[0]]\n",
    "        all_num_edges += [edge_index.shape[1]]\n",
    "        all_density += [(2 * all_num_edges[-1]) / (all_num_nodes[-1] * (all_num_nodes[-1] - 1))]\n",
    "        all_num_labels += [subset_local_adj_matrix.sum()]\n",
    "\n",
    "        subset_local_adj_matrix = subset_local_adj_matrix / subset_local_adj_matrix.sum()\n",
    "        local_rel_excess_homophily = (subset_local_adj_matrix - label_prop) / (1 - label_prop)\n",
    "\n",
    "        rel_excess_homophily[l] += [local_rel_excess_homophily[l].item()]\n",
    "\n",
    "        #if i == max_iterations-1:\n",
    "        #    break\n",
    "\n",
    "networkx_graph = torch_geometric.utils.to_networkx(\n",
    "    graph_dataset[\"graph\"],\n",
    "    to_undirected=True\n",
    "    )\n",
    "\n",
    "degree_centrality = nx.degree_centrality(networkx_graph)\n",
    "degree_centrality = [degree_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "eigen_centrality = nx.eigenvector_centrality_numpy(networkx_graph, max_iter=10)\n",
    "eigen_centrality = [eigen_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "del graph_dataset, networkx_graph\n",
    "\n",
    "graph_stats = {\n",
    "    \"mean_rel_excess_homophily\": rel_excess_homophily,\n",
    "    \"num_nodes\": all_num_nodes,\n",
    "    \"num_edges\": all_num_edges,\n",
    "    \"num_labels\": all_num_labels,\n",
    "    \"graph_density\": all_density,\n",
    "    \"label_density\": [(num_labels / num_nodes).item() for num_labels, num_nodes in zip(all_num_labels, all_num_nodes)],\n",
    "    \"degree_centrality\": degree_centrality,\n",
    "    \"eigen_centrality\": eigen_centrality,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(graph_stats['mean_rel_excess_homophily'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_local_adj_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HealthStory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"./data/structured/HealthStory/seed[942]_splits[5]_minlen[0]_filterisolated[True]_topk[20]_topexcl[0]_userdoc[30]_featuretype[one-hot]_vocab[joint][random][10000x768]_userfeatures[post][zeros]/0\"\n",
    "\n",
    "subdirs = next(os.walk(loc))[1]\n",
    "for subdir in subdirs:\n",
    "    \n",
    "    split = re.search(\"split\\=(.+?), \", subdir).group(1)\n",
    "    \n",
    "    version  = re.search(\"version\\=(.+?)\\)\", subdir)\n",
    "    \n",
    "    if version is None:\n",
    "        continue\n",
    "    else:\n",
    "        meta_split = version.group(1)\n",
    "    \n",
    "    if split == \"train\" and meta_split == \"meta_train_support\":\n",
    "        support_batches_loc = loc + f\"/{subdir}\"\n",
    "    elif split == \"train\" and meta_split == \"meta_train_query\":\n",
    "        query_batches_loc = loc + f\"/{subdir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_graph_stats = []\n",
    "\n",
    "batch_locs = next(os.walk(support_batches_loc))[2]\n",
    "for i, batched_graph in enumerate(tqdm(batch_locs)):\n",
    "    \n",
    "    batched_graph_loc = support_batches_loc + f\"/{batched_graph}\"\n",
    "    \n",
    "    graph = torch.load(batched_graph_loc, map_location=\"cpu\")\n",
    "\n",
    "    support_graph_stats += [compute_graph_stats(graph)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "records = list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 0,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][0],\n",
    "            },\n",
    "        healthstory_support_stats,\n",
    "        )\n",
    "    )\n",
    "\n",
    "records += list(\n",
    "    map(\n",
    "        lambda x: {\n",
    "            \"label\": 1,\n",
    "            \"excess_homophily\": x[\"mean_rel_excess_homophily\"][1],\n",
    "            },\n",
    "        healthstory_support_stats,\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=ax,\n",
    "    palette=\"tab10\"\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"\")\n",
    "#ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "\n",
    "ax.set_ylabel(\"HealthStory\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "#ax.set_title(\"Support\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/healthstory_support.pdf\")\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/healthstory_support.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/healthstory_support.pickle\", \"wb\") as f:\n",
    "    pickle.dump(support_graph_stats, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/structured/HealthStory/seed[942]_splits[5]_minlen[0]_filterisolated[True]_topk[20]_topexcl[0]_userdoc[30]_featuretype[one-hot]_vocab[joint][random][10000x768]_userfeatures[post][zeros]/0/socialgraph(mode=inductive, split=train, keep_cc=largest).pickle\", \"rb\") as f:\n",
    "    graph_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = graph_dataset[\"num_classes\"]\n",
    "\n",
    "graph_labels = graph_dataset[\"graph\"][\"y\"]\n",
    "label_locs = [torch.where(graph_labels == l)[0] for l in range(num_classes)]\n",
    "all_label_locs = torch.cat(label_locs)\n",
    "\n",
    "label_prop = torch.tensor(list(map(lambda x: x.shape[0], label_locs)))\n",
    "label_prop = label_prop / label_prop.sum()\n",
    "\n",
    "num_nodes = graph_dataset[\"graph\"].num_nodes\n",
    "\n",
    "all_num_nodes = []\n",
    "all_num_edges = []\n",
    "all_density = []\n",
    "all_num_labels = []\n",
    "\n",
    "rel_excess_homophily = {l: [] for l in range(num_classes)}\n",
    "\n",
    "max_iterations = min(label_locs[l].shape[0] for l in range(num_classes))\n",
    "\n",
    "for l in range(num_classes):\n",
    "    for i, label_loc in enumerate(label_locs[l]):\n",
    "        \n",
    "        subset, edge_index, _, _ = k_hop_subgraph(\n",
    "            node_idx=label_loc.unsqueeze(0),\n",
    "            edge_index=graph_dataset[\"graph\"][\"edge_index\"],\n",
    "            num_hops=2,\n",
    "        )\n",
    "\n",
    "        subset_local_adj_matrix = torch.zeros((num_classes, ))\n",
    "\n",
    "        for ll in range(num_classes):\n",
    "\n",
    "            ll_label_nodes = torch.isin(\n",
    "                test_elements=subset,\n",
    "                elements=label_locs[ll],\n",
    "                assume_unique=True,\n",
    "            ).sum()\n",
    "\n",
    "            subset_local_adj_matrix[ll] += ll_label_nodes\n",
    "\n",
    "        all_num_nodes += [subset.shape[0]]\n",
    "        all_num_edges += [edge_index.shape[1]]\n",
    "        all_density += [(2 * all_num_edges[-1]) / (all_num_nodes[-1] * (all_num_nodes[-1] - 1))]\n",
    "        all_num_labels += [subset_local_adj_matrix.sum()]\n",
    "\n",
    "        subset_local_adj_matrix = subset_local_adj_matrix / subset_local_adj_matrix.sum()\n",
    "        local_rel_excess_homophily = (subset_local_adj_matrix - label_prop) / (1 - label_prop)\n",
    "\n",
    "        rel_excess_homophily[l] += [local_rel_excess_homophily[l].item()]\n",
    "\n",
    "        #if i == max_iterations-1:\n",
    "        #    break\n",
    "\n",
    "\n",
    "networkx_graph = torch_geometric.utils.to_networkx(\n",
    "    graph_dataset[\"graph\"],\n",
    "    to_undirected=True\n",
    "    )\n",
    "\n",
    "degree_centrality = nx.degree_centrality(networkx_graph)\n",
    "degree_centrality = [degree_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "eigen_centrality = nx.eigenvector_centrality_numpy(networkx_graph, max_iter=10)\n",
    "eigen_centrality = [eigen_centrality[i] for i in all_label_locs.tolist()]\n",
    "\n",
    "del graph_dataset, networkx_graph\n",
    "\n",
    "graph_stats = {\n",
    "    \"mean_rel_excess_homophily\": rel_excess_homophily,\n",
    "    \"num_nodes\": all_num_nodes,\n",
    "    \"num_edges\": all_num_edges,\n",
    "    \"num_labels\": all_num_labels,\n",
    "    \"graph_density\": all_density,\n",
    "    \"label_density\": [(num_labels / num_nodes).item() for num_labels, num_nodes in zip(all_num_labels, all_num_nodes)],\n",
    "    \"degree_centrality\": degree_centrality,\n",
    "    \"eigen_centrality\": eigen_centrality,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"label\": l, \"excess_homophily\": metric_val}\n",
    "    for l, metric_vals in rel_excess_homophily.items()\n",
    "    for metric_val in metric_vals\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette(\"tab10\"),\n",
    "    common_norm=False,\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"\")\n",
    "#ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "ax.get_legend().remove()\n",
    "\n",
    "ax.set_ylabel(\"\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "#ax.set_title(\"Query\", fontsize=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/healthstory_query.pdf\")\n",
    "fig.savefig(\"../misc/figures/rel_excess_homophily/healthstory_query.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/healthstory_query.pickle\", \"wb\") as f:\n",
    "    pickle.dump(graph_stats, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {\"label\": l, \"excess_homophily\": metric_val}\n",
    "    for l, metric_vals in rel_excess_homophily.items()\n",
    "    for metric_val in metric_vals\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6.30 / 2, 9.72 / 3))\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=df,\n",
    "    x=\"excess_homophily\",\n",
    "    hue=\"label\",\n",
    "    stat=\"probability\",\n",
    "    fill=True,\n",
    "    multiple=\"dodge\",\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette(\"tab10\"),\n",
    "    common_norm=False,\n",
    "    )\n",
    "\n",
    "ax.set_xlim([-1.25, 1.25])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"\")\n",
    "#ax.set_xlabel(\"Rel. Excess Homophily\")\n",
    "ax.get_legend().remove()\n",
    "\n",
    "ax.set_ylabel(\"\", fontsize=11)\n",
    "ax.set_yticks([])\n",
    "\n",
    "#ax.set_title(\"Query\", fontsize=11)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/gossipcop_support.pickle\", \"rb\") as f:\n",
    "    gossipcop_support_stats = pickle.load(f)\n",
    "\n",
    "with open(\"../misc/stats/gossipcop_query.pickle\", \"rb\") as f:\n",
    "    gossipcop_query_stats = pickle.load(f)\n",
    "\n",
    "with open(\"../misc/stats/coaid_support.pickle\", \"rb\") as f:\n",
    "    coaid_support_stats = pickle.load(f)\n",
    "\n",
    "with open(\"../misc/stats/coaid_query.pickle\", \"rb\") as f:\n",
    "    coaid_query_stats = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../misc/stats/twitterhatespeech_support.pickle\", \"rb\") as f:\n",
    "    twitter_support_stats = pickle.load(f)\n",
    "\n",
    "with open(\"../misc/stats/twitterhatespeech_query.pickle\", \"rb\") as f:\n",
    "    twitter_query_stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def make_support_df(stats, num_labels: int = 2):\n",
    "    records = []\n",
    "    \n",
    "    for l in range(num_labels):\n",
    "        records += list(\n",
    "            map(\n",
    "                lambda x: {\n",
    "                    \"label\": l,\n",
    "                    \"excess_homophily\": x[\"mean_rel_excess_homophily\"][l],\n",
    "                    },\n",
    "                stats\n",
    "                )\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_query_df(stats):\n",
    "    records = [\n",
    "        {\"label\": l, \"excess_homophily\": metric_val}\n",
    "        for l, metric_vals in stats[\"mean_rel_excess_homophily\"].items()\n",
    "        for metric_val in metric_vals\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatches = ['', '\\\\\\\\', '//']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=figsize)\n",
    "flat_axes = np.ravel(axes)\n",
    "\n",
    "axes[0, 0].set_title(\"Support\", fontsize=11)\n",
    "axes[0, 1].set_title(\"Query\", fontsize=11)\n",
    "\n",
    "# Gossipcop\n",
    "axes[0, 0] = sns.kdeplot(\n",
    "    data=make_support_df(gossipcop_support_stats, num_labels=2),\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=axes[0, 0],\n",
    "    palette=\"tab10\",\n",
    "    label=\"label\",\n",
    "    )\n",
    "axes[0, 0].set_ylabel(\"Gossipcop\", fontsize=11)\n",
    "\n",
    "handles = []\n",
    "for collection, handle, hatch in zip(axes[0, 0].collections[::-1], axes[0, 0].get_legend().legend_handles, hatches):\n",
    "    collection.set_hatch(hatch)\n",
    "    handle.set_hatch(hatch)\n",
    "    \n",
    "    handles.append(handle)\n",
    "\n",
    "axes[0, 0].legend(\n",
    "    handles=handles,\n",
    "    labels=[\"Real\", \"Fake\"],\n",
    "    loc='upper left',\n",
    "    title=\"\"\n",
    "    )\n",
    "\n",
    "axes[0, 1] = sns.kdeplot(\n",
    "    data=make_query_df(gossipcop_query_stats),\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=axes[0, 1],\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "axes[0, 1].set_ylabel(\"\", fontsize=11)\n",
    "axes[0, 1].get_legend().remove()\n",
    "\n",
    "for collection, hatch in zip(axes[0, 1].collections[::-1], hatches):\n",
    "    collection.set_hatch(hatch)\n",
    "\n",
    "# TwitterHateSpeech\n",
    "axes[1, 0] = sns.kdeplot(\n",
    "    data=make_support_df(coaid_support_stats, num_labels=2),\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=axes[1, 0],\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "axes[1, 0].set_ylabel(\"CoAID\", fontsize=11)\n",
    "\n",
    "handles = []\n",
    "for collection, handle, hatch in zip(axes[1, 0].collections[::-1], axes[1, 0].get_legend().legend_handles, hatches):\n",
    "    collection.set_hatch(hatch)\n",
    "    handle.set_hatch(hatch)\n",
    "    \n",
    "    handles.append(handle)\n",
    "\n",
    "axes[1, 0].legend(\n",
    "    handles=handles,\n",
    "    labels=[\"Real\", \"Fake\"],\n",
    "    loc='upper left',\n",
    "    title=\"\"\n",
    "    )\n",
    "\n",
    "axes[1, 1] = sns.kdeplot(\n",
    "    data=make_query_df(coaid_query_stats),\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=axes[1, 1],\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "axes[1, 1].set_ylabel(\"\", fontsize=11)\n",
    "axes[1, 1].get_legend().remove()\n",
    "\n",
    "for collection, hatch in zip(axes[1, 1].collections[::-1], hatches):\n",
    "    collection.set_hatch(hatch)\n",
    "\n",
    "# TwitterHS \n",
    "axes[2, 0] = sns.kdeplot(\n",
    "    data=make_support_df(twitter_support_stats, num_labels=3),\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=axes[2, 0],\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "axes[2, 0].set_ylabel(\"TwitterHateSpeech\", fontsize=11)\n",
    "\n",
    "handles = []\n",
    "for collection, handle, hatch in zip(axes[2, 0].collections[::-1], axes[2, 0].get_legend().legend_handles, hatches):\n",
    "    collection.set_hatch(hatch)\n",
    "    handle.set_hatch(hatch)\n",
    "    \n",
    "    handles.append(handle)\n",
    "\n",
    "axes[2, 0].legend(\n",
    "    handles=handles,\n",
    "    labels=[\"Racism\", \"Sexism\", \"None\"],\n",
    "    loc='upper left',\n",
    "    title=\"\"\n",
    "    )\n",
    "\n",
    "axes[2, 1] = sns.kdeplot(\n",
    "    data=make_query_df(twitter_query_stats),\n",
    "    x=\"excess_homophily\",\n",
    "    bw_adjust=0.8,\n",
    "    clip=(-10, 1),\n",
    "    hue=\"label\",\n",
    "    fill=True,\n",
    "    multiple=\"layer\",\n",
    "    ax=axes[2, 1],\n",
    "    palette=\"tab10\",\n",
    "    )\n",
    "axes[2, 1].set_ylabel(\"\", fontsize=11)\n",
    "axes[2, 1].get_legend().remove()\n",
    "\n",
    "for collection, hatch in zip(axes[2, 1].collections[::-1], hatches):\n",
    "    collection.set_hatch(hatch)\n",
    "\n",
    "for ax in flat_axes:\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([-1.25, 1.25])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "axes[2, 0].set_xticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "axes[2, 1].set_xticklabels([-1, -0.5, 0, 0.5, 1])\n",
    "fig.supxlabel(\"Rel. Excess Homophily\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\n",
    "    \"../../meta-learning-gnns-paper/emnlp2023-latex/figures/homophily_plot.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[2, 0].legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def five_stats(values: list):\n",
    "    \n",
    "    values = np.array(values)\n",
    "    \n",
    "    quantiles = np.quantile(values, q=[0.25, 0.50, 0.75])\n",
    "    \n",
    "    five_stats_summary = {\n",
    "        \"mean\": np.mean(values),\n",
    "        \"stddev\": np.std(values), \n",
    "        \"q25\": quantiles[0],\n",
    "        \"q50\": quantiles[1],\n",
    "        \"q75\": quantiles[2],\n",
    "\n",
    "    }\n",
    "    \n",
    "    return five_stats_summary\n",
    "\n",
    "def get_stats_table(stats_records):\n",
    "\n",
    "    if isinstance(next(iter(stats_records)), dict):\n",
    "        stats = defaultdict(list)\n",
    "\n",
    "        for record in stats_records:\n",
    "            for k, v in record.items():\n",
    "\n",
    "                if isinstance(v, list):\n",
    "                    for kk, vv in enumerate(v):\n",
    "                        stats[f\"{k}_{kk}\"].append(vv)\n",
    "\n",
    "                else:\n",
    "                        stats[k] += [v]\n",
    "\n",
    "    else:\n",
    "        stats = stats_records\n",
    "\n",
    "    records = []\n",
    "    for k, v in stats.items():\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                records.append({\"metric\": f\"{k}_{kk}\", **five_stats(vv)})\n",
    "\n",
    "        else:\n",
    "            records.append({\"metric\": k, **five_stats(v)})\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    print(df.to_string())\n",
    "    df.to_clipboard(excel=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_table(gossipcop_support_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_table(gossipcop_query_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_table(coaid_support_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_table(coaid_query_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_table(twitter_support_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_table(twitter_query_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-learning-gnns-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53a5447bd48efa6055b33d19680aa678d1747ea89831be2839fa4f19e2db5dcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
